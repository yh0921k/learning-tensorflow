{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9c4083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-433964a587db>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/guest/anaconda3/envs/learning/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/guest/anaconda3/envs/learning/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/guest/anaconda3/envs/learning/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/guest/anaconda3/envs/learning/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/guest/anaconda3/envs/learning/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-433964a587db>:28: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch:  0001 cost =  5.745170979\n",
      "Epoch:  0002 cost =  1.780056703\n",
      "Epoch:  0003 cost =  1.122778632\n",
      "Epoch:  0004 cost =  0.872012254\n",
      "Epoch:  0005 cost =  0.738203180\n",
      "Epoch:  0006 cost =  0.654728893\n",
      "Epoch:  0007 cost =  0.596023616\n",
      "Epoch:  0008 cost =  0.552216826\n",
      "Epoch:  0009 cost =  0.518254966\n",
      "Epoch:  0010 cost =  0.491113202\n",
      "Epoch:  0011 cost =  0.468347538\n",
      "Epoch:  0012 cost =  0.449374354\n",
      "Epoch:  0013 cost =  0.432675662\n",
      "Epoch:  0014 cost =  0.418828159\n",
      "Epoch:  0015 cost =  0.406128936\n",
      "Epoch:  0016 cost =  0.394982946\n",
      "Epoch:  0017 cost =  0.385870423\n",
      "Epoch:  0018 cost =  0.376135586\n",
      "Epoch:  0019 cost =  0.368269381\n",
      "Epoch:  0020 cost =  0.361209774\n",
      "Epoch:  0021 cost =  0.354798146\n",
      "Epoch:  0022 cost =  0.348525127\n",
      "Epoch:  0023 cost =  0.342752730\n",
      "Epoch:  0024 cost =  0.337285917\n",
      "Epoch:  0025 cost =  0.332443594\n",
      "Epoch:  0026 cost =  0.327556533\n",
      "Epoch:  0027 cost =  0.324047232\n",
      "Epoch:  0028 cost =  0.319670900\n",
      "Epoch:  0029 cost =  0.315536209\n",
      "Epoch:  0030 cost =  0.312257760\n",
      "Epoch:  0031 cost =  0.308550811\n",
      "Epoch:  0032 cost =  0.305987613\n",
      "Epoch:  0033 cost =  0.302624458\n",
      "Epoch:  0034 cost =  0.299895895\n",
      "Epoch:  0035 cost =  0.297245874\n",
      "Epoch:  0036 cost =  0.294490169\n",
      "Epoch:  0037 cost =  0.292061211\n",
      "Epoch:  0038 cost =  0.290009241\n",
      "Epoch:  0039 cost =  0.287633525\n",
      "Epoch:  0040 cost =  0.285644497\n",
      "Epoch:  0041 cost =  0.283856603\n",
      "Epoch:  0042 cost =  0.281824815\n",
      "Epoch:  0043 cost =  0.280098974\n",
      "Epoch:  0044 cost =  0.278386739\n",
      "Epoch:  0045 cost =  0.276589560\n",
      "Epoch:  0046 cost =  0.275093705\n",
      "Epoch:  0047 cost =  0.273444052\n",
      "Epoch:  0048 cost =  0.271918684\n",
      "Epoch:  0049 cost =  0.270640437\n",
      "Epoch:  0050 cost =  0.269054379\n",
      "Finished\n",
      "Accuracy:  0.9194\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(num_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    print('Epoch: ', '%04d' % (epoch + 1), 'cost = ', '{:.9f}'.format(avg_cost))\n",
    "print(\"Finished\")\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce47e4a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:  0001 cost =  170.616951048\n",
      "Epoch:  0002 cost =  40.565238302\n",
      "Epoch:  0003 cost =  25.518340862\n",
      "Epoch:  0004 cost =  17.682265345\n",
      "Epoch:  0005 cost =  12.712052231\n",
      "Epoch:  0006 cost =  9.350572426\n",
      "Epoch:  0007 cost =  6.906868887\n",
      "Epoch:  0008 cost =  5.084565194\n",
      "Epoch:  0009 cost =  3.780364261\n",
      "Epoch:  0010 cost =  2.862635394\n",
      "Epoch:  0011 cost =  2.024928279\n",
      "Epoch:  0012 cost =  1.556260941\n",
      "Epoch:  0013 cost =  1.095696701\n",
      "Epoch:  0014 cost =  0.882209005\n",
      "Epoch:  0015 cost =  0.765814629\n",
      "Epoch:  0016 cost =  0.599051516\n",
      "Epoch:  0017 cost =  0.521750829\n",
      "Epoch:  0018 cost =  0.421651028\n",
      "Epoch:  0019 cost =  0.463649412\n",
      "Epoch:  0020 cost =  0.390237665\n",
      "Epoch:  0021 cost =  0.353477148\n",
      "Epoch:  0022 cost =  0.277936661\n",
      "Epoch:  0023 cost =  0.349300354\n",
      "Epoch:  0024 cost =  0.305526673\n",
      "Epoch:  0025 cost =  0.287328805\n",
      "Epoch:  0026 cost =  0.239089998\n",
      "Epoch:  0027 cost =  0.233890165\n",
      "Epoch:  0028 cost =  0.275216120\n",
      "Epoch:  0029 cost =  0.320138243\n",
      "Epoch:  0030 cost =  0.209471280\n",
      "Epoch:  0031 cost =  0.236021360\n",
      "Epoch:  0032 cost =  0.193507564\n",
      "Epoch:  0033 cost =  0.245334886\n",
      "Epoch:  0034 cost =  0.214266939\n",
      "Epoch:  0035 cost =  0.147433776\n",
      "Epoch:  0036 cost =  0.193197925\n",
      "Epoch:  0037 cost =  0.179760433\n",
      "Epoch:  0038 cost =  0.161819819\n",
      "Epoch:  0039 cost =  0.189847252\n",
      "Epoch:  0040 cost =  0.187424912\n",
      "Epoch:  0041 cost =  0.177645421\n",
      "Epoch:  0042 cost =  0.128236833\n",
      "Epoch:  0043 cost =  0.160059510\n",
      "Epoch:  0044 cost =  0.204409247\n",
      "Epoch:  0045 cost =  0.142951663\n",
      "Epoch:  0046 cost =  0.190960741\n",
      "Epoch:  0047 cost =  0.169556764\n",
      "Epoch:  0048 cost =  0.152494839\n",
      "Epoch:  0049 cost =  0.167976905\n",
      "Epoch:  0050 cost =  0.202828284\n",
      "Finished\n",
      "Accuracy:  0.9583\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(num_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    print('Epoch: ', '%04d' % (epoch + 1), 'cost = ', '{:.9f}'.format(avg_cost))\n",
    "print(\"Finished\")\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ef7d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Epoch:  0001 cost =  0.294813841\n",
      "Epoch:  0002 cost =  0.111888904\n",
      "Epoch:  0003 cost =  0.072573902\n",
      "Epoch:  0004 cost =  0.051651657\n",
      "Epoch:  0005 cost =  0.038197389\n",
      "Epoch:  0006 cost =  0.029556795\n",
      "Epoch:  0007 cost =  0.025734275\n",
      "Epoch:  0008 cost =  0.019396474\n",
      "Epoch:  0009 cost =  0.015436155\n",
      "Epoch:  0010 cost =  0.016188128\n",
      "Epoch:  0011 cost =  0.012033268\n",
      "Epoch:  0012 cost =  0.011698458\n",
      "Epoch:  0013 cost =  0.010695566\n",
      "Epoch:  0014 cost =  0.010347069\n",
      "Epoch:  0015 cost =  0.008343381\n",
      "Epoch:  0016 cost =  0.013325912\n",
      "Epoch:  0017 cost =  0.005537319\n",
      "Epoch:  0018 cost =  0.005691460\n",
      "Epoch:  0019 cost =  0.008951609\n",
      "Epoch:  0020 cost =  0.008379006\n",
      "Epoch:  0021 cost =  0.005702116\n",
      "Epoch:  0022 cost =  0.007387497\n",
      "Epoch:  0023 cost =  0.004667923\n",
      "Epoch:  0024 cost =  0.008448539\n",
      "Epoch:  0025 cost =  0.004456693\n",
      "Epoch:  0026 cost =  0.008042067\n",
      "Epoch:  0027 cost =  0.006479734\n",
      "Epoch:  0028 cost =  0.001645189\n",
      "Epoch:  0029 cost =  0.007741921\n",
      "Epoch:  0030 cost =  0.006247372\n",
      "Epoch:  0031 cost =  0.006946901\n",
      "Epoch:  0032 cost =  0.005211719\n",
      "Epoch:  0033 cost =  0.003630952\n",
      "Epoch:  0034 cost =  0.006239684\n",
      "Epoch:  0035 cost =  0.003474461\n",
      "Epoch:  0036 cost =  0.002516228\n",
      "Epoch:  0037 cost =  0.009506209\n",
      "Epoch:  0038 cost =  0.004919958\n",
      "Epoch:  0039 cost =  0.001498253\n",
      "Epoch:  0040 cost =  0.004662458\n",
      "Epoch:  0041 cost =  0.007851614\n",
      "Epoch:  0042 cost =  0.004103061\n",
      "Epoch:  0043 cost =  0.005330617\n",
      "Epoch:  0044 cost =  0.000689463\n",
      "Epoch:  0045 cost =  0.000054688\n",
      "Epoch:  0046 cost =  0.000016683\n",
      "Epoch:  0047 cost =  0.000011986\n",
      "Epoch:  0048 cost =  0.000009489\n",
      "Epoch:  0049 cost =  0.000007680\n",
      "Epoch:  0050 cost =  0.000006251\n",
      "Finished\n",
      "Accuracy:  0.9838\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(num_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    print('Epoch: ', '%04d' % (epoch + 1), 'cost = ', '{:.9f}'.format(avg_cost))\n",
    "print(\"Finished\")\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "514f2d08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-2e8f7ad7f689>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/guest/anaconda3/envs/learning/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/guest/anaconda3/envs/learning/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/guest/anaconda3/envs/learning/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/guest/anaconda3/envs/learning/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/guest/anaconda3/envs/learning/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-1-2e8f7ad7f689>:43: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch:  0001 cost =  0.297034095\n",
      "Epoch:  0002 cost =  0.106105656\n",
      "Epoch:  0003 cost =  0.071410818\n",
      "Epoch:  0004 cost =  0.053053888\n",
      "Epoch:  0005 cost =  0.041523620\n",
      "Epoch:  0006 cost =  0.036626457\n",
      "Epoch:  0007 cost =  0.030500260\n",
      "Epoch:  0008 cost =  0.024197739\n",
      "Epoch:  0009 cost =  0.022209465\n",
      "Epoch:  0010 cost =  0.021292652\n",
      "Epoch:  0011 cost =  0.020107870\n",
      "Epoch:  0012 cost =  0.019944036\n",
      "Epoch:  0013 cost =  0.016497322\n",
      "Epoch:  0014 cost =  0.015145286\n",
      "Epoch:  0015 cost =  0.015415358\n",
      "Epoch:  0016 cost =  0.014906026\n",
      "Epoch:  0017 cost =  0.013385776\n",
      "Epoch:  0018 cost =  0.009389980\n",
      "Epoch:  0019 cost =  0.010575880\n",
      "Epoch:  0020 cost =  0.011235164\n",
      "Epoch:  0021 cost =  0.009995023\n",
      "Epoch:  0022 cost =  0.012245929\n",
      "Epoch:  0023 cost =  0.008663790\n",
      "Epoch:  0024 cost =  0.010545731\n",
      "Epoch:  0025 cost =  0.011104600\n",
      "Epoch:  0026 cost =  0.010109441\n",
      "Epoch:  0027 cost =  0.009129671\n",
      "Epoch:  0028 cost =  0.009567331\n",
      "Epoch:  0029 cost =  0.008582977\n",
      "Epoch:  0030 cost =  0.008887130\n",
      "Epoch:  0031 cost =  0.008178443\n",
      "Epoch:  0032 cost =  0.008127763\n",
      "Epoch:  0033 cost =  0.008074176\n",
      "Epoch:  0034 cost =  0.006456121\n",
      "Epoch:  0035 cost =  0.009145626\n",
      "Epoch:  0036 cost =  0.007235377\n",
      "Epoch:  0037 cost =  0.006692062\n",
      "Epoch:  0038 cost =  0.007351069\n",
      "Epoch:  0039 cost =  0.009423655\n",
      "Epoch:  0040 cost =  0.006488372\n",
      "Epoch:  0041 cost =  0.006770623\n",
      "Epoch:  0042 cost =  0.006294304\n",
      "Epoch:  0043 cost =  0.006269585\n",
      "Epoch:  0044 cost =  0.007416620\n",
      "Epoch:  0045 cost =  0.005785321\n",
      "Epoch:  0046 cost =  0.008309054\n",
      "Epoch:  0047 cost =  0.003618608\n",
      "Epoch:  0048 cost =  0.011229568\n",
      "Epoch:  0049 cost =  0.008637474\n",
      "Epoch:  0050 cost =  0.003799561\n",
      "Finished\n",
      "Accuracy:  0.9829\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(num_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    print('Epoch: ', '%04d' % (epoch + 1), 'cost = ', '{:.9f}'.format(avg_cost))\n",
    "print(\"Finished\")\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda3c4cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-d398402586c5>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/guest/anaconda3/envs/learning/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/guest/anaconda3/envs/learning/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/guest/anaconda3/envs/learning/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/guest/anaconda3/envs/learning/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/guest/anaconda3/envs/learning/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-1-d398402586c5>:22: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-1-d398402586c5>:50: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch:  0001 cost =  0.469759704\n",
      "Epoch:  0002 cost =  0.170478593\n",
      "Epoch:  0003 cost =  0.131393887\n",
      "Epoch:  0004 cost =  0.106725184\n",
      "Epoch:  0005 cost =  0.090742748\n",
      "Epoch:  0006 cost =  0.080758169\n",
      "Epoch:  0007 cost =  0.073537847\n",
      "Epoch:  0008 cost =  0.068404451\n",
      "Epoch:  0009 cost =  0.061329646\n",
      "Epoch:  0010 cost =  0.059435682\n",
      "Epoch:  0011 cost =  0.056064263\n",
      "Epoch:  0012 cost =  0.051047447\n",
      "Epoch:  0013 cost =  0.050206377\n",
      "Epoch:  0014 cost =  0.046429730\n",
      "Epoch:  0015 cost =  0.046248964\n",
      "Epoch:  0016 cost =  0.044289836\n",
      "Epoch:  0017 cost =  0.046643845\n",
      "Epoch:  0018 cost =  0.038956684\n",
      "Epoch:  0019 cost =  0.037882772\n",
      "Epoch:  0020 cost =  0.038335601\n",
      "Epoch:  0021 cost =  0.036944205\n",
      "Epoch:  0022 cost =  0.033505151\n",
      "Epoch:  0023 cost =  0.036120222\n",
      "Epoch:  0024 cost =  0.033310525\n",
      "Epoch:  0025 cost =  0.034764730\n",
      "Epoch:  0026 cost =  0.036344912\n",
      "Epoch:  0027 cost =  0.033225393\n",
      "Epoch:  0028 cost =  0.029959204\n",
      "Epoch:  0029 cost =  0.028804653\n",
      "Epoch:  0030 cost =  0.028141439\n",
      "Epoch:  0031 cost =  0.030113958\n",
      "Epoch:  0032 cost =  0.031797756\n",
      "Epoch:  0033 cost =  0.029733341\n",
      "Epoch:  0034 cost =  0.030407826\n",
      "Epoch:  0035 cost =  0.025245459\n",
      "Epoch:  0036 cost =  0.026971881\n",
      "Epoch:  0037 cost =  0.029982393\n",
      "Epoch:  0038 cost =  0.027326134\n",
      "Epoch:  0039 cost =  0.030444804\n",
      "Epoch:  0040 cost =  0.025225473\n",
      "Epoch:  0041 cost =  0.027117053\n",
      "Epoch:  0042 cost =  0.025662891\n",
      "Epoch:  0043 cost =  0.027203360\n",
      "Epoch:  0044 cost =  0.025742647\n",
      "Epoch:  0045 cost =  0.025613264\n",
      "Epoch:  0046 cost =  0.021423211\n",
      "Epoch:  0047 cost =  0.024836663\n",
      "Epoch:  0048 cost =  0.023229299\n",
      "Epoch:  0049 cost =  0.029805403\n",
      "Epoch:  0050 cost =  0.024870919\n",
      "Finished\n",
      "Accuracy:  0.9838\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout(keep_prob) rate 0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(num_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    print('Epoch: ', '%04d' % (epoch + 1), 'cost = ', '{:.9f}'.format(avg_cost))\n",
    "print(\"Finished\")\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18bb517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
