{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c2a501b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.8743463 [[-0.4259668  -0.20594999 -0.33208537]\n",
      " [ 0.81815857  1.5363392  -1.4941758 ]\n",
      " [-0.12596472 -1.4122726   1.5989274 ]]\n",
      "1 2.283538 [[-0.4359366  -0.2050738  -0.32299176]\n",
      " [ 0.77955216  1.5609298  -1.48016   ]\n",
      " [-0.15451677 -1.3385159   1.5537227 ]]\n",
      "2 2.2197433 [[-0.4361631  -0.21448523 -0.31335384]\n",
      " [ 0.79850006  1.5258228  -1.4640009 ]\n",
      " [-0.13060237 -1.3194196   1.5107121 ]]\n",
      "3 2.1786726 [[-0.4413075  -0.22023755 -0.30245712]\n",
      " [ 0.7892367   1.5124084  -1.4413232 ]\n",
      " [-0.13325284 -1.2807314   1.4746745 ]]\n",
      "4 2.1416516 [[-0.4443984  -0.22836077 -0.291243  ]\n",
      " [ 0.792328    1.4851931  -1.4171993 ]\n",
      " [-0.12442021 -1.2551451   1.4402554 ]]\n",
      "5 2.105853 [[-0.44887483 -0.23541562 -0.27971172]\n",
      " [ 0.7876318   1.4641805  -1.3914905 ]\n",
      " [-0.12279473 -1.2241751   1.4076599 ]]\n",
      "6 2.070578 [[-0.45277405 -0.24315    -0.26807812]\n",
      " [ 0.78653556  1.4390991  -1.3653128 ]\n",
      " [-0.11771225 -1.1972917   1.375694  ]]\n",
      "7 2.035644 [[-0.45714667 -0.25049883 -0.25635666]\n",
      " [ 0.78288335  1.416124   -1.3386854 ]\n",
      " [-0.11488353 -1.1687874   1.3443608 ]]\n",
      "8 2.0009985 [[-0.46135086 -0.2580373  -0.24461399]\n",
      " [ 0.7803949   1.3918722  -1.3119451 ]\n",
      " [-0.11083095 -1.141782    1.313303  ]]\n",
      "9 1.9666301 [[-0.46574694 -0.26539576 -0.23285945]\n",
      " [ 0.77695733  1.368477   -1.2851123 ]\n",
      " [-0.10751347 -1.1143094   1.2825129 ]]\n",
      "10 1.9325416 [[-0.47010398 -0.27278107 -0.22111708]\n",
      " [ 0.7739079   1.3447167  -1.2583025 ]\n",
      " [-0.10368446 -1.0874993   1.2518737 ]]\n",
      "11 1.8987389 [[-0.47455657 -0.280053   -0.20939256]\n",
      " [ 0.77045625  1.3213934  -1.2315274 ]\n",
      " [-0.10007767 -1.060617    1.2213846 ]]\n",
      "12 1.8652306 [[-0.47901472 -0.28729123 -0.19769615]\n",
      " [ 0.7671226   1.2980305  -1.2048309 ]\n",
      " [-0.09621099 -1.034107    1.191008  ]]\n",
      "13 1.8320253 [[-0.4835324  -0.29443777 -0.18603192]\n",
      " [ 0.7635891   1.2749532  -1.17822   ]\n",
      " [-0.09238159 -1.0076759   1.1607474 ]]\n",
      "14 1.799133 [[-0.48807174 -0.30152482 -0.17440554]\n",
      " [ 0.7600733   1.251963   -1.1517141 ]\n",
      " [-0.08839051 -0.9815129   1.1305933 ]]\n",
      "15 1.7665633 [[-0.49265558 -0.30852598 -0.16282056]\n",
      " [ 0.7564393   1.2292018  -1.1253189 ]\n",
      " [-0.08436731 -0.9554944   1.1005516 ]]\n",
      "16 1.7343271 [[-0.49726665 -0.31545436 -0.15128113]\n",
      " [ 0.7527851   1.2065822  -1.099045  ]\n",
      " [-0.08022532 -0.92970777  1.0706229 ]]\n",
      "17 1.702435 [[-0.5019149  -0.32229653 -0.1397907 ]\n",
      " [ 0.7490497   1.184171   -1.0728985 ]\n",
      " [-0.07602546 -0.9040991   1.0408144 ]]\n",
      "18 1.6708981 [[-0.5065918  -0.32905743 -0.12835295]\n",
      " [ 0.7452807   1.1619287  -1.0468872 ]\n",
      " [-0.07172877 -0.87871253  1.0111312 ]]\n",
      "19 1.6397278 [[-0.5113015  -0.33572933 -0.11697137]\n",
      " [ 0.74145025  1.1398891  -1.0210172 ]\n",
      " [-0.06736624 -0.85352486  0.981581  ]]\n",
      "20 1.6089363 [[-0.51603925 -0.34231347 -0.10564947]\n",
      " [ 0.73758334  1.1180346  -0.99529576]\n",
      " [-0.0629206  -0.8285607   0.95217115]]\n",
      "21 1.5785356 [[-0.52080667 -0.3488048  -0.09439074]\n",
      " [ 0.7336676   1.096384   -0.9697295 ]\n",
      " [-0.05840933 -0.8038113   0.92291045]]\n",
      "22 1.5485381 [[-0.5256007  -0.3552029  -0.08319861]\n",
      " [ 0.72971773  1.07493    -0.9443255 ]\n",
      " [-0.05382545 -0.77929246  0.8938077 ]]\n",
      "23 1.5189565 [[-0.5304216  -0.3615041  -0.07207648]\n",
      " [ 0.72572875  1.0536844  -0.9190909 ]\n",
      " [-0.0491803  -0.75500256  0.86487263]]\n",
      "24 1.4898041 [[-0.53526723 -0.36770728 -0.06102766]\n",
      " [ 0.7217103   1.032645   -0.89403313]\n",
      " [-0.04447218 -0.73095334  0.83611524]]\n",
      "25 1.4610939 [[-0.54013723 -0.37380958 -0.0500554 ]\n",
      " [ 0.71766126  1.0118204  -0.8691596 ]\n",
      " [-0.03970964 -0.70714694  0.8075463 ]]\n",
      "26 1.4328395 [[-0.5450297  -0.37980968 -0.0391628 ]\n",
      " [ 0.7135889   0.9912111  -0.8444779 ]\n",
      " [-0.03489371 -0.6835936   0.779177  ]]\n",
      "27 1.4050546 [[-0.549944   -0.38570532 -0.02835286]\n",
      " [ 0.7094939   0.97082424 -0.819996  ]\n",
      " [-0.03003186 -0.6602975   0.7510191 ]]\n",
      "28 1.3777525 [[-0.5548785  -0.3914953  -0.01762839]\n",
      " [ 0.7053824   0.95066166 -0.79572195]\n",
      " [-0.0251267  -0.6372685   0.7230849 ]]\n",
      "29 1.3509474 [[-0.5598323  -0.39717782 -0.00699206]\n",
      " [ 0.7012562   0.9307299  -0.77166396]\n",
      " [-0.02018513 -0.61451226  0.6953872 ]]\n",
      "30 1.3246529 [[-0.5648039  -0.40275195  0.00355368]\n",
      " [ 0.69712055  0.9110322  -0.74783057]\n",
      " [-0.01521098 -0.59203833  0.66793907]]\n",
      "31 1.2988827 [[-0.5697923  -0.40821648  0.01400657]\n",
      " [ 0.692978    0.89157456 -0.7242304 ]\n",
      " [-0.01021063 -0.5698539   0.6407543 ]]\n",
      "32 1.2736502 [[-0.574796   -0.41357076  0.02436458]\n",
      " [ 0.6888333   0.8723613  -0.7008724 ]\n",
      " [-0.00518869 -0.5479683   0.6138468 ]]\n",
      "33 1.248969 [[-5.7981396e-01 -4.1881412e-01  3.4625914e-02]\n",
      " [ 6.8468946e-01  8.5339844e-01 -6.7776573e-01]\n",
      " [-1.5124073e-04 -5.2638978e-01  5.8723074e-01]]\n",
      "34 1.2248518 [[-0.58484477 -0.42394638  0.04478901]\n",
      " [ 0.6805508   0.83469105 -0.6549196 ]\n",
      " [ 0.0048968  -0.5051278   0.5609207 ]]\n",
      "35 1.2013111 [[-0.58988726 -0.4289674   0.05485253]\n",
      " [ 0.67642045  0.8162455  -0.6323437 ]\n",
      " [ 0.00994954 -0.48419097  0.5349312 ]]\n",
      "36 1.1783589 [[-0.59494007 -0.43387744  0.0648154 ]\n",
      " [ 0.67230266  0.7980674  -0.61004776]\n",
      " [ 0.0150022  -0.46358907  0.5092766 ]]\n",
      "37 1.1560065 [[-0.600002   -0.4386769   0.07467677]\n",
      " [ 0.66820055  0.7801635  -0.5880417 ]\n",
      " [ 0.02004929 -0.44333094  0.48397142]]\n",
      "38 1.1342642 [[-0.6050716  -0.44336653  0.08443603]\n",
      " [ 0.6641179   0.7625402  -0.56633574]\n",
      " [ 0.02508618 -0.42342603  0.45902961]]\n",
      "39 1.1131413 [[-0.6101476  -0.44794726  0.09409276]\n",
      " [ 0.66005796  0.74520445 -0.54494005]\n",
      " [ 0.03010812 -0.40388325  0.4344649 ]]\n",
      "40 1.0926461 [[-0.6152286  -0.4524203   0.10364677]\n",
      " [ 0.65602434  0.72816306 -0.52386504]\n",
      " [ 0.03511095 -0.38471147  0.4102903 ]]\n",
      "41 1.0727859 [[-0.6203132  -0.45678696  0.11309807]\n",
      " [ 0.65202004  0.71142334 -0.503121  ]\n",
      " [ 0.04009067 -0.36591908  0.38651818]]\n",
      "42 1.0535656 [[-0.62540007 -0.4610489   0.12244684]\n",
      " [ 0.6480484   0.6949923  -0.48271832]\n",
      " [ 0.04504384 -0.34751412  0.36316007]]\n",
      "43 1.0349898 [[-0.6304877  -0.46520787  0.13169342]\n",
      " [ 0.64411235  0.6788771  -0.4626671 ]\n",
      " [ 0.04996737 -0.32950404  0.34022644]]\n",
      "44 1.0170605 [[-0.63557464 -0.46926585  0.14083834]\n",
      " [ 0.6402147   0.663085   -0.44297734]\n",
      " [ 0.05485858 -0.31189552  0.3177267 ]]\n",
      "45 0.9997784 [[-0.64065945 -0.47322494  0.14988224]\n",
      " [ 0.63635826  0.64762264 -0.4236586 ]\n",
      " [ 0.05971538 -0.2946947   0.29566908]]\n",
      "46 0.9831419 [[-0.6457407  -0.47708738  0.15882592]\n",
      " [ 0.6325455   0.6324969  -0.40472007]\n",
      " [ 0.06453591 -0.27790666  0.27406052]]\n",
      "47 0.96714795 [[-0.65081686 -0.48085555  0.16767028]\n",
      " [ 0.6287789   0.61771375 -0.38617033]\n",
      " [ 0.06931902 -0.26153585  0.2529066 ]]\n",
      "48 0.9517912 [[-0.6558866  -0.48453188  0.17641635]\n",
      " [ 0.6250603   0.60327923 -0.36801732]\n",
      " [ 0.07406359 -0.24558535  0.23221153]]\n",
      "49 0.9370645 [[-0.66094846 -0.48811895  0.18506528]\n",
      " [ 0.621392    0.5891983  -0.35026813]\n",
      " [ 0.07876926 -0.23005772  0.21197823]]\n",
      "50 0.922959 [[-0.66600114 -0.49161932  0.19361831]\n",
      " [ 0.61777556  0.5754757  -0.33292904]\n",
      " [ 0.08343565 -0.21495403  0.19220814]]\n",
      "51 0.90946394 [[-0.6710433  -0.49503565  0.2020768 ]\n",
      " [ 0.61421245  0.56211513 -0.31600535]\n",
      " [ 0.08806285 -0.20027459  0.1729015 ]]\n",
      "52 0.8965666 [[-0.6760736  -0.49837065  0.21044216]\n",
      " [ 0.61070406  0.5491195  -0.29950127]\n",
      " [ 0.09265124 -0.18601868  0.15405719]]\n",
      "53 0.88425314 [[-0.681091   -0.50162697  0.21871592]\n",
      " [ 0.60725117  0.5364911  -0.28342   ]\n",
      " [ 0.09720109 -0.17218427  0.13567294]]\n",
      "54 0.87250805 [[-0.68609434 -0.5048074   0.2268997 ]\n",
      " [ 0.6038549   0.52423096 -0.26776356]\n",
      " [ 0.10171323 -0.15876883  0.11774536]]\n",
      "55 0.86131454 [[-0.69108266 -0.50791454  0.23499517]\n",
      " [ 0.60051554  0.5123396  -0.25253284]\n",
      " [ 0.10618813 -0.14576839  0.10027002]]\n",
      "56 0.85065496 [[-0.696055   -0.5109511   0.24300407]\n",
      " [ 0.59723365  0.5008162  -0.23772763]\n",
      " [ 0.11062674 -0.13317856  0.08324157]]\n",
      "57 0.8405105 [[-0.7010106  -0.51391965  0.25092822]\n",
      " [ 0.5940093   0.48965952 -0.22334656]\n",
      " [ 0.11502972 -0.12099376  0.06665381]]\n",
      "58 0.83086205 [[-0.70594865 -0.5168228   0.25876945]\n",
      " [ 0.59084255  0.4788669  -0.20938721]\n",
      " [ 0.11939794 -0.10920798  0.05049981]]\n",
      "59 0.82168967 [[-0.7108686  -0.5196631   0.26652965]\n",
      " [ 0.5877331   0.46843532 -0.19584616]\n",
      " [ 0.12373202 -0.09781425  0.03477199]]\n",
      "60 0.81297326 [[-0.7157698  -0.52244294  0.27421072]\n",
      " [ 0.58468074  0.45836052 -0.18271898]\n",
      " [ 0.12803286 -0.08680532  0.01946223]]\n",
      "61 0.8046925 [[-0.720652   -0.52516466  0.28181458]\n",
      " [ 0.58168477  0.4486379  -0.1700004 ]\n",
      " [ 0.13230093 -0.07617308  0.00456193]]\n",
      "62 0.7968272 [[-0.72551465 -0.5278306   0.28934318]\n",
      " [ 0.5787448   0.4392618  -0.15768437]\n",
      " [ 0.136537   -0.06590933 -0.00993789]]\n",
      "63 0.7893573 [[-0.7303576  -0.5304429   0.29679844]\n",
      " [ 0.57586     0.4302263  -0.14576405]\n",
      " [ 0.14074142 -0.05600515 -0.0240465 ]]\n",
      "64 0.7822628 [[-0.7351806  -0.5330037   0.30418226]\n",
      " [ 0.57302964  0.4215246  -0.13423201]\n",
      " [ 0.14491479 -0.04645162 -0.03777341]]\n",
      "65 0.77552426 [[-0.7399836  -0.535515    0.3114966 ]\n",
      " [ 0.5702528   0.4131497  -0.12308028]\n",
      " [ 0.1490574  -0.03723934 -0.05112829]]\n",
      "66 0.76912284 [[-0.74476653 -0.53797877  0.31874326]\n",
      " [ 0.56752855  0.40509406 -0.11230041]\n",
      " [ 0.15316959 -0.02835891 -0.06412093]]\n",
      "67 0.76304007 [[-0.7495293  -0.5403968   0.3259241 ]\n",
      " [ 0.564856    0.3973498  -0.10188359]\n",
      " [ 0.15725164 -0.01980072 -0.07676117]]\n",
      "68 0.7572582 [[-0.75427204 -0.54277086  0.33304092]\n",
      " [ 0.5622341   0.3899088  -0.09182068]\n",
      " [ 0.16130373 -0.01155513 -0.08905885]]\n",
      "69 0.75176 [[-0.7589949  -0.5451026   0.3400955 ]\n",
      " [ 0.55966187  0.3827627  -0.08210234]\n",
      " [ 0.165326   -0.00361246 -0.1010238 ]]\n",
      "70 0.7465291 [[-0.763698   -0.5473935   0.3470895 ]\n",
      " [ 0.55713826  0.37590304 -0.07271904]\n",
      " [ 0.16931856  0.00403693 -0.11266576]]\n",
      "71 0.74154985 [[-0.7683815  -0.5496451   0.3540246 ]\n",
      " [ 0.5546623   0.36932114 -0.06366119]\n",
      " [ 0.1732815   0.01140259 -0.12399437]]\n",
      "72 0.73680735 [[-0.7730456  -0.5518588   0.36090237]\n",
      " [ 0.5522329   0.3630084  -0.05491911]\n",
      " [ 0.17721486  0.01849404 -0.13501917]]\n",
      "73 0.7322873 [[-0.7776906  -0.5540358   0.3677244 ]\n",
      " [ 0.5498492   0.35695615 -0.04648313]\n",
      " [ 0.18111862  0.0253206  -0.1457495 ]]\n",
      "74 0.7279762 [[-0.78231674 -0.5561774   0.37449214]\n",
      " [ 0.54751015  0.3511558  -0.03834367]\n",
      " [ 0.18499276  0.03189152 -0.15619454]]\n",
      "75 0.72386134 [[-0.78692436 -0.5582847   0.38120708]\n",
      " [ 0.5452148   0.34559867 -0.03049119]\n",
      " [ 0.1888373   0.03821576 -0.16636331]]\n",
      "76 0.71993077 [[-0.7915137  -0.5603588   0.38787058]\n",
      " [ 0.54296213  0.34027642 -0.0229163 ]\n",
      " [ 0.19265212  0.04430225 -0.17626463]]\n",
      "77 0.7161732 [[-0.79608506 -0.5624008   0.39448395]\n",
      " [ 0.5407514   0.33518058 -0.01560974]\n",
      " [ 0.1964373   0.0501595  -0.18590707]]\n",
      "78 0.71257794 [[-0.80063885 -0.5644116   0.4010485 ]\n",
      " [ 0.5385816   0.33030307 -0.00856243]\n",
      " [ 0.2001927   0.05579605 -0.19529901]]\n",
      "79 0.70913476 [[-0.8051753  -0.56639206  0.40756544]\n",
      " [ 0.536452    0.32563576 -0.0017655 ]\n",
      " [ 0.20391832  0.06122002 -0.20444861]]\n",
      "80 0.7058347 [[-0.8096948  -0.56834304  0.41403595]\n",
      " [ 0.5343617   0.3211708   0.00478974]\n",
      " [ 0.20761412  0.06643938 -0.21336377]]\n",
      "81 0.70266855 [[-0.8141977  -0.5702653   0.42046112]\n",
      " [ 0.5323099   0.31690064  0.01111175]\n",
      " [ 0.21127999  0.07146192 -0.22205219]]\n",
      "82 0.6996283 [[-0.81868434 -0.57215965  0.42684206]\n",
      " [ 0.53029597  0.31281757  0.01720878]\n",
      " [ 0.2149161   0.07629495 -0.23052132]]\n",
      "83 0.6967061 [[-0.823155   -0.5740267   0.4331798 ]\n",
      " [ 0.52831894  0.3089146   0.02308879]\n",
      " [ 0.21852219  0.08094596 -0.23877841]]\n",
      "84 0.6938948 [[-0.82761    -0.5758672   0.4394753 ]\n",
      " [ 0.52637833  0.30518442  0.02875957]\n",
      " [ 0.22209847  0.0854217  -0.24683043]]\n",
      "85 0.69118774 [[-0.8320498  -0.57768166  0.44572952]\n",
      " [ 0.5244733   0.30162045  0.03422858]\n",
      " [ 0.2256448   0.08972915 -0.2546842 ]]\n",
      "86 0.6885785 [[-0.83647454 -0.57947075  0.4519434 ]\n",
      " [ 0.52260333  0.29821587  0.03950315]\n",
      " [ 0.2291613   0.09387464 -0.2623462 ]]\n",
      "87 0.68606126 [[-0.8408847  -0.581235    0.45811778]\n",
      " [ 0.5207676   0.29496443  0.04459025]\n",
      " [ 0.23264791  0.09786465 -0.26982284]]\n",
      "88 0.68363065 [[-0.84528047 -0.5829749   0.46425352]\n",
      " [ 0.5189658   0.29185984  0.0494967 ]\n",
      " [ 0.23610483  0.10170507 -0.27712017]]\n",
      "89 0.68128127 [[-0.84966224 -0.584691    0.47035137]\n",
      " [ 0.51719695  0.28889632  0.05422903]\n",
      " [ 0.23953187  0.10540194 -0.28424412]]\n",
      "90 0.6790086 [[-0.8540303  -0.5863837   0.47641212]\n",
      " [ 0.51546085  0.28606787  0.05879356]\n",
      " [ 0.24292943  0.10896063 -0.29120037]]\n",
      "91 0.6768081 [[-0.85838497 -0.58805346  0.4824365 ]\n",
      " [ 0.5137567   0.28336924  0.06319636]\n",
      " [ 0.24629731  0.11238681 -0.29799443]]\n",
      "92 0.67467564 [[-0.86272645 -0.5897007   0.48842525]\n",
      " [ 0.51208407  0.2807949   0.06744333]\n",
      " [ 0.24963579  0.11568549 -0.3046316 ]]\n",
      "93 0.6726073 [[-0.86705506 -0.5913258   0.494379  ]\n",
      " [ 0.5104424   0.2783398   0.07154008]\n",
      " [ 0.25294486  0.11886178 -0.31111696]]\n",
      "94 0.67059934 [[-0.8713711  -0.5929292   0.50029844]\n",
      " [ 0.5088312   0.275999    0.07549204]\n",
      " [ 0.25622475  0.12192043 -0.3174555 ]]\n",
      "95 0.66864866 [[-0.8756748  -0.5945112   0.50618416]\n",
      " [ 0.5072501   0.27376777  0.07930445]\n",
      " [ 0.2594756   0.12486606 -0.32365197]]\n",
      "96 0.66675186 [[-0.87996644 -0.5960722   0.5120368 ]\n",
      " [ 0.5056984   0.27164158  0.08298233]\n",
      " [ 0.26269746  0.12770316 -0.32971093]]\n",
      "97 0.6649062 [[-0.88424623 -0.5976125   0.5178569 ]\n",
      " [ 0.5041758   0.269616    0.08653053]\n",
      " [ 0.2658906   0.13043588 -0.3356368 ]]\n",
      "98 0.66310865 [[-0.88851446 -0.5991324   0.52364504]\n",
      " [ 0.50268173  0.2676869   0.08995368]\n",
      " [ 0.2690551   0.13306841 -0.34143382]]\n",
      "99 0.66135705 [[-0.8927713  -0.60063225  0.5294017 ]\n",
      " [ 0.5012159   0.2658502   0.09325625]\n",
      " [ 0.27219126  0.13560459 -0.34710616]]\n",
      "100 0.6596486 [[-0.89701706 -0.6021123   0.5351275 ]\n",
      " [ 0.49977767  0.2641021   0.09644254]\n",
      " [ 0.27529913  0.13804828 -0.35265774]]\n",
      "101 0.65798134 [[-0.9012519  -0.60357285  0.54082286]\n",
      " [ 0.4983668   0.2624388   0.09951669]\n",
      " [ 0.27837908  0.14040296 -0.35809237]]\n",
      "102 0.656353 [[-0.90547603 -0.60501415  0.5464883 ]\n",
      " [ 0.49698278  0.26085684  0.10248267]\n",
      " [ 0.2814312   0.1426722  -0.36341375]]\n",
      "103 0.65476173 [[-0.90968966 -0.6064365   0.55212426]\n",
      " [ 0.49562523  0.25935277  0.1053443 ]\n",
      " [ 0.28445575  0.14485928 -0.3686254 ]]\n",
      "104 0.6532056 [[-0.913893   -0.6078401   0.5577312 ]\n",
      " [ 0.49429375  0.2579233   0.10810523]\n",
      " [ 0.287453    0.1469674  -0.37373075]]\n",
      "105 0.6516829 [[-0.91808623 -0.6092253   0.5633096 ]\n",
      " [ 0.4929879   0.2565654   0.11076902]\n",
      " [ 0.2904231   0.14899965 -0.3787331 ]]\n",
      "106 0.6501921 [[-0.9222695  -0.6105922   0.5688598 ]\n",
      " [ 0.49170738  0.25527588  0.11333904]\n",
      " [ 0.2933664   0.15095885 -0.3836356 ]]\n",
      "107 0.6487316 [[-0.92644304 -0.6119411   0.5743823 ]\n",
      " [ 0.4904517   0.25405207  0.11581855]\n",
      " [ 0.29628304  0.15284799 -0.38844135]]\n",
      "108 0.6473 [[-0.93060696 -0.61327225  0.57987744]\n",
      " [ 0.4892206   0.25289106  0.11821069]\n",
      " [ 0.29917336  0.15466961 -0.39315328]]\n",
      "109 0.64589596 [[-0.9347615  -0.6145859   0.58534557]\n",
      " [ 0.4880136   0.2517903   0.12051848]\n",
      " [ 0.30203754  0.15642637 -0.39777425]]\n",
      "110 0.64451826 [[-0.9389068  -0.61588216  0.5907871 ]\n",
      " [ 0.48683035  0.25074717  0.1227448 ]\n",
      " [ 0.30487588  0.15812075 -0.402307  ]]\n",
      "111 0.6431658 [[-0.94304293 -0.6171613   0.59620243]\n",
      " [ 0.4856705   0.24975936  0.12489247]\n",
      " [ 0.30768868  0.15975516 -0.4067542 ]]\n",
      "112 0.6418373 [[-0.94717014 -0.6184235   0.6015918 ]\n",
      " [ 0.4845338   0.2488244   0.12696414]\n",
      " [ 0.31047624  0.16133177 -0.4111184 ]]\n",
      "113 0.64053196 [[-0.9512885  -0.619669    0.6069557 ]\n",
      " [ 0.4834197   0.24794024  0.12896241]\n",
      " [ 0.3132387   0.16285296 -0.41540202]]\n",
      "114 0.6392485 [[-0.9553982  -0.620898    0.6122944 ]\n",
      " [ 0.48232797  0.2471046   0.13088979]\n",
      " [ 0.31597647  0.16432068 -0.41960752]]\n",
      "115 0.6379863 [[-0.9594993  -0.62211066  0.61760813]\n",
      " [ 0.48125827  0.24631548  0.13274862]\n",
      " [ 0.31868982  0.16573697 -0.42373717]]\n",
      "116 0.63674426 [[-0.963592   -0.62330717  0.6228973 ]\n",
      " [ 0.48021016  0.24557096  0.13454124]\n",
      " [ 0.32137895  0.16710384 -0.42779317]]\n",
      "117 0.63552177 [[-0.96767634 -0.6244877   0.6281622 ]\n",
      " [ 0.4791834   0.24486908  0.13626985]\n",
      " [ 0.32404426  0.16842306 -0.4317777 ]]\n",
      "118 0.63431776 [[-0.9717525  -0.62565243  0.6334031 ]\n",
      " [ 0.4781776   0.24420816  0.13793659]\n",
      " [ 0.32668594  0.16969648 -0.4356928 ]]\n",
      "119 0.633132 [[-0.9758206  -0.62680155  0.6386204 ]\n",
      " [ 0.4771925   0.24358636  0.13954352]\n",
      " [ 0.32930437  0.17092575 -0.43954048]]\n",
      "120 0.6319632 [[-0.97988075 -0.62793523  0.6438142 ]\n",
      " [ 0.47622764  0.24300212  0.1410926 ]\n",
      " [ 0.33189973  0.17211257 -0.4433227 ]]\n",
      "121 0.6308112 [[-0.983933   -0.62905365  0.6489849 ]\n",
      " [ 0.47528285  0.24245375  0.14258575]\n",
      " [ 0.33447245  0.17325842 -0.44704127]]\n",
      "122 0.6296751 [[-0.98797745 -0.630157    0.6541327 ]\n",
      " [ 0.47435763  0.2419399   0.14402482]\n",
      " [ 0.3370227   0.17436497 -0.45069808]]\n",
      "123 0.6285545 [[-0.9920142  -0.63124543  0.65925795]\n",
      " [ 0.47345182  0.24145895  0.14541158]\n",
      " [ 0.33955085  0.17543352 -0.4542948 ]]\n",
      "124 0.62744874 [[-0.99604344 -0.6323191   0.6643608 ]\n",
      " [ 0.47256503  0.24100962  0.14674771]\n",
      " [ 0.34205717  0.17646554 -0.45783314]]\n",
      "125 0.6263574 [[-1.0000652  -0.63337815  0.66944164]\n",
      " [ 0.47169694  0.24059054  0.14803489]\n",
      " [ 0.34454194  0.17746237 -0.46131474]]\n",
      "126 0.6252797 [[-1.0040796  -0.6344228   0.6745006 ]\n",
      " [ 0.47084728  0.2402004   0.14927468]\n",
      " [ 0.3470055   0.17842524 -0.46474117]]\n",
      "127 0.62421554 [[-1.0080866  -0.6354531   0.6795379 ]\n",
      " [ 0.47001567  0.23983803  0.15046863]\n",
      " [ 0.34944806  0.17935544 -0.46811393]]\n",
      "128 0.6231643 [[-1.0120863  -0.6364693   0.68455386]\n",
      " [ 0.46920192  0.23950219  0.15161821]\n",
      " [ 0.35187     0.18025409 -0.47143453]]\n",
      "129 0.6221257 [[-1.0160788  -0.6374715   0.6895487 ]\n",
      " [ 0.4684056   0.23919187  0.15272486]\n",
      " [ 0.3542715   0.18112242 -0.47470438]]\n",
      "130 0.6210991 [[-1.0200644  -0.63845986  0.69452256]\n",
      " [ 0.46762654  0.23890588  0.15378994]\n",
      " [ 0.356653    0.18196143 -0.47792488]]\n",
      "131 0.6200843 [[-1.0240428  -0.6394346   0.69947577]\n",
      " [ 0.46686432  0.23864326  0.15481478]\n",
      " [ 0.3590147   0.18277223 -0.48109734]]\n",
      "132 0.6190808 [[-1.0280144  -0.64039576  0.70440847]\n",
      " [ 0.46611875  0.23840295  0.15580066]\n",
      " [ 0.3613569   0.18355578 -0.4842231 ]]\n",
      "133 0.61808836 [[-1.0319791  -0.64134353  0.7093209 ]\n",
      " [ 0.46538946  0.23818405  0.15674883]\n",
      " [ 0.36367986  0.18431309 -0.48730335]]\n",
      "134 0.6171067 [[-1.035937   -0.6422781   0.71421325]\n",
      " [ 0.46467623  0.23798564  0.15766047]\n",
      " [ 0.3659839   0.18504506 -0.49033937]]\n",
      "135 0.61613536 [[-1.039888   -0.6431995   0.71908575]\n",
      " [ 0.46397874  0.23780684  0.15853675]\n",
      " [ 0.3682693   0.18575259 -0.4933323 ]]\n",
      "136 0.6151742 [[-1.0438324  -0.64410794  0.7239386 ]\n",
      " [ 0.46329674  0.23764682  0.15937877]\n",
      " [ 0.37053633  0.18643653 -0.49628326]]\n",
      "137 0.61422276 [[-1.0477701  -0.64500356  0.728772  ]\n",
      " [ 0.4626299   0.2375048   0.16018762]\n",
      " [ 0.3727852   0.18709776 -0.4991934 ]]\n",
      "138 0.6132808 [[-1.0517013  -0.6458865   0.7335861 ]\n",
      " [ 0.46197802  0.23737995  0.16096434]\n",
      " [ 0.3750163   0.18773699 -0.50206375]]\n",
      "139 0.61234826 [[-1.0556259  -0.6467568   0.7383811 ]\n",
      " [ 0.4613408   0.23727156  0.16170993]\n",
      " [ 0.37722987  0.188355   -0.5048953 ]]\n",
      "140 0.61142474 [[-1.0595441  -0.6476148   0.7431572 ]\n",
      " [ 0.46071795  0.237179    0.16242537]\n",
      " [ 0.37942612  0.18895261 -0.5076892 ]]\n",
      "141 0.61050993 [[-1.0634558  -0.6484604   0.7479146 ]\n",
      " [ 0.46010926  0.23710147  0.16311158]\n",
      " [ 0.38160542  0.18953042 -0.5104463 ]]\n",
      "142 0.60960376 [[-1.0673612  -0.64929384  0.7526535 ]\n",
      " [ 0.4595144   0.2370384   0.1637695 ]\n",
      " [ 0.38376796  0.19008917 -0.5131676 ]]\n",
      "143 0.6087059 [[-1.0712602  -0.65011525  0.7573739 ]\n",
      " [ 0.4589332   0.23698911  0.1644    ]\n",
      " [ 0.38591406  0.19062945 -0.515854  ]]\n",
      "144 0.6078162 [[-1.075153   -0.65092474  0.7620762 ]\n",
      " [ 0.4583653   0.2369531   0.16500394]\n",
      " [ 0.38804388  0.19115198 -0.51850635]]\n",
      "145 0.6069344 [[-1.0790395  -0.6517225   0.76676047]\n",
      " [ 0.45781055  0.23692966  0.16558212]\n",
      " [ 0.3901578   0.19165725 -0.52112556]]\n",
      "146 0.6060604 [[-1.0829198  -0.65250856  0.77142686]\n",
      " [ 0.4572686   0.23691836  0.16613536]\n",
      " [ 0.39225593  0.19214594 -0.5237124 ]]\n",
      "147 0.605194 [[-1.086794   -0.6532831   0.77607554]\n",
      " [ 0.45673937  0.23691855  0.16666439]\n",
      " [ 0.39433873  0.1926185  -0.52626777]]\n",
      "148 0.60433483 [[-1.0906621  -0.65404624  0.7807067 ]\n",
      " [ 0.4562224   0.23692992  0.16717   ]\n",
      " [ 0.3964062   0.19307566 -0.5287924 ]]\n",
      "149 0.60348296 [[-1.0945241  -0.6547981   0.7853205 ]\n",
      " [ 0.45571768  0.23695174  0.16765289]\n",
      " [ 0.39845884  0.19351766 -0.531287  ]]\n",
      "150 0.60263824 [[-1.0983801  -0.65553874  0.7899171 ]\n",
      " [ 0.4552248   0.23698378  0.16811375]\n",
      " [ 0.4004967   0.19394524 -0.53375244]]\n",
      "151 0.6018003 [[-1.1022301  -0.6562683   0.79449666]\n",
      " [ 0.45474362  0.23702541  0.1685533 ]\n",
      " [ 0.4025201   0.19435874 -0.5361894 ]]\n",
      "152 0.6009691 [[-1.1060741  -0.65698695  0.7990593 ]\n",
      " [ 0.45427388  0.2370763   0.16897215]\n",
      " [ 0.40452927  0.19475867 -0.5385985 ]]\n",
      "153 0.6001446 [[-1.1099122  -0.65769476  0.80360514]\n",
      " [ 0.4538154   0.23713599  0.16937095]\n",
      " [ 0.40652448  0.19514544 -0.54098046]]\n",
      "154 0.5993265 [[-1.1137443  -0.65839183  0.8081344 ]\n",
      " [ 0.45336786  0.23720418  0.16975032]\n",
      " [ 0.40850586  0.19551957 -0.5433359 ]]\n",
      "155 0.5985147 [[-1.1175705  -0.6590783   0.81264716]\n",
      " [ 0.45293117  0.23728034  0.17011085]\n",
      " [ 0.41047376  0.19588132 -0.54566556]]\n",
      "156 0.5977091 [[-1.1213909  -0.6597543   0.8171436 ]\n",
      " [ 0.452505    0.23736423  0.17045313]\n",
      " [ 0.4124283   0.19623122 -0.54797   ]]\n",
      "157 0.59690964 [[-1.1252056  -0.6604199   0.82162386]\n",
      " [ 0.4520892   0.23745546  0.17077771]\n",
      " [ 0.41436976  0.19656956 -0.5502498 ]]\n",
      "158 0.59611607 [[-1.1290145  -0.66107523  0.8260881 ]\n",
      " [ 0.45168355  0.23755369  0.17108512]\n",
      " [ 0.41629836  0.19689676 -0.5525056 ]]\n",
      "159 0.5953283 [[-1.1328176  -0.6617204   0.83053637]\n",
      " [ 0.45128787  0.2376586   0.1713759 ]\n",
      " [ 0.4182143   0.19721314 -0.5547379 ]]\n",
      "160 0.5945463 [[-1.136615   -0.6623555   0.83496886]\n",
      " [ 0.45090193  0.2377699   0.17165056]\n",
      " [ 0.4201178   0.19751905 -0.55694735]]\n",
      "161 0.59376997 [[-1.1404067  -0.6629806   0.83938575]\n",
      " [ 0.45052552  0.23788728  0.17190957]\n",
      " [ 0.42200908  0.19781482 -0.5591344 ]]\n",
      "162 0.59299904 [[-1.1441928  -0.6635959   0.84378713]\n",
      " [ 0.45015845  0.2380105   0.17215344]\n",
      " [ 0.4238883   0.1981008  -0.5612996 ]]\n",
      "163 0.5922336 [[-1.1479733  -0.66420144  0.84817314]\n",
      " [ 0.44980058  0.2381392   0.17238262]\n",
      " [ 0.4257557   0.19837724 -0.5634435 ]]\n",
      "164 0.59147334 [[-1.1517482  -0.6647973   0.8525439 ]\n",
      " [ 0.44945166  0.23827317  0.17259757]\n",
      " [ 0.4276115   0.19864443 -0.5655665 ]]\n",
      "165 0.5907185 [[-1.1555175  -0.66538364  0.8568995 ]\n",
      " [ 0.4491115   0.23841222  0.1727987 ]\n",
      " [ 0.42945585  0.19890273 -0.5676691 ]]\n",
      "166 0.5899687 [[-1.1592811  -0.66596055  0.86124015]\n",
      " [ 0.44878     0.23855598  0.17298643]\n",
      " [ 0.43128902  0.19915229 -0.5697518 ]]\n",
      "167 0.58922386 [[-1.1630393  -0.6665281   0.8655659 ]\n",
      " [ 0.44845685  0.23870438  0.1731612 ]\n",
      " [ 0.4331111   0.19939353 -0.5718151 ]]\n",
      "168 0.5884839 [[-1.166792   -0.66708636  0.8698769 ]\n",
      " [ 0.44814202  0.23885706  0.17332338]\n",
      " [ 0.43492237  0.19962655 -0.57385933]]\n",
      "169 0.587749 [[-1.1705393  -0.6676355   0.8741733 ]\n",
      " [ 0.4478352   0.23901387  0.17347339]\n",
      " [ 0.43672293  0.19985166 -0.57588506]]\n",
      "170 0.5870187 [[-1.174281   -0.6681756   0.8784551 ]\n",
      " [ 0.44753632  0.23917457  0.17361157]\n",
      " [ 0.43851304  0.20006908 -0.5778926 ]]\n",
      "171 0.5862932 [[-1.1780174  -0.66870666  0.88272256]\n",
      " [ 0.44724512  0.23933904  0.1737383 ]\n",
      " [ 0.44029284  0.2002791  -0.5798824 ]]\n",
      "172 0.58557224 [[-1.1817483  -0.66922885  0.8869757 ]\n",
      " [ 0.44696155  0.23950697  0.17385393]\n",
      " [ 0.44206256  0.20048182 -0.5818548 ]]\n",
      "173 0.5848559 [[-1.1854738  -0.6697423   0.8912147 ]\n",
      " [ 0.4466853   0.23967835  0.17395881]\n",
      " [ 0.44382223  0.20067759 -0.58381027]]\n",
      "174 0.584144 [[-1.189194   -0.670247    0.8954397 ]\n",
      " [ 0.44641638  0.23985283  0.17405327]\n",
      " [ 0.4455722   0.20086646 -0.5857491 ]]\n",
      "175 0.5834365 [[-1.1929089  -0.67074317  0.8996507 ]\n",
      " [ 0.44615448  0.2400304   0.1741376 ]\n",
      " [ 0.4473125   0.20104878 -0.5876717 ]]\n",
      "176 0.5827334 [[-1.1966184  -0.6712308   0.9038479 ]\n",
      " [ 0.44589958  0.24021076  0.17421216]\n",
      " [ 0.44904342  0.20122461 -0.58957845]]\n",
      "177 0.5820345 [[-1.2003226  -0.67170995  0.9080313 ]\n",
      " [ 0.44565138  0.2403939   0.17427723]\n",
      " [ 0.45076498  0.20139426 -0.59146965]]\n",
      "178 0.5813398 [[-1.2040216  -0.67218083  0.9122011 ]\n",
      " [ 0.4454099   0.2405795   0.17433311]\n",
      " [ 0.4524775   0.20155774 -0.59334564]]\n",
      "179 0.58064926 [[-1.2077153  -0.6726434   0.9163574 ]\n",
      " [ 0.4451748   0.24076764  0.17438006]\n",
      " [ 0.454181    0.20171544 -0.5952068 ]]\n",
      "180 0.57996273 [[-1.2114037  -0.67309785  0.9205003 ]\n",
      " [ 0.44494617  0.24095796  0.17441839]\n",
      " [ 0.45587572  0.20186728 -0.5970534 ]]\n",
      "181 0.5792803 [[-1.2150869  -0.67354417  0.92462987]\n",
      " [ 0.44472364  0.24115053  0.17444836]\n",
      " [ 0.4575617   0.20201364 -0.5988858 ]]\n",
      "182 0.57860184 [[-1.218765   -0.6739825   0.9287463 ]\n",
      " [ 0.44450724  0.24134503  0.17447025]\n",
      " [ 0.45923927  0.20215447 -0.60070413]]\n",
      "183 0.5779272 [[-1.2224379  -0.6744129   0.9328496 ]\n",
      " [ 0.4442967   0.24154158  0.17448427]\n",
      " [ 0.46090838  0.20229015 -0.6025089 ]]\n",
      "184 0.57725656 [[-1.2261056  -0.6748355   0.9369399 ]\n",
      " [ 0.44409204  0.24173981  0.1744907 ]\n",
      " [ 0.46256936  0.20242056 -0.6043003 ]]\n",
      "185 0.57658947 [[-1.2297682  -0.6752503   0.94101727]\n",
      " [ 0.44389293  0.24193986  0.17448977]\n",
      " [ 0.46422213  0.20254612 -0.6060786 ]]\n",
      "186 0.5759264 [[-1.2334256  -0.67565745  0.94508183]\n",
      " [ 0.44369948  0.24214137  0.1744817 ]\n",
      " [ 0.4658671   0.20266667 -0.6078441 ]]\n",
      "187 0.57526684 [[-1.237078   -0.676057    0.9491337 ]\n",
      " [ 0.44351134  0.24234448  0.17446674]\n",
      " [ 0.46750414  0.20278262 -0.6095971 ]]\n",
      "188 0.57461095 [[-1.2407252  -0.676449    0.953173  ]\n",
      " [ 0.44332856  0.2425489   0.17444511]\n",
      " [ 0.4691336   0.20289385 -0.6113378 ]]\n",
      "189 0.57395875 [[-1.2443674  -0.67683357  0.95719975]\n",
      " [ 0.44315094  0.24275464  0.17441699]\n",
      " [ 0.4707555   0.20300063 -0.61306643]]\n",
      "190 0.57331 [[-1.2480046  -0.67721075  0.96121407]\n",
      " [ 0.4429783   0.24296165  0.17438263]\n",
      " [ 0.4723699   0.2031031  -0.6147833 ]]\n",
      "191 0.57266474 [[-1.2516366  -0.67758065  0.96521604]\n",
      " [ 0.44281065  0.2431697   0.1743422 ]\n",
      " [ 0.47397712  0.2032012  -0.61648864]]\n",
      "192 0.572023 [[-1.2552637  -0.6779433   0.9692058 ]\n",
      " [ 0.44264776  0.24337888  0.17429592]\n",
      " [ 0.4755771   0.20329525 -0.61818266]]\n",
      "193 0.57138455 [[-1.2588857  -0.6782988   0.9731834 ]\n",
      " [ 0.44248965  0.24358894  0.17424396]\n",
      " [ 0.47717008  0.20338516 -0.6198656 ]]\n",
      "194 0.5707496 [[-1.2625029  -0.67864716  0.97714895]\n",
      " [ 0.44233608  0.24379995  0.17418651]\n",
      " [ 0.4787561   0.2034712  -0.6215376 ]]\n",
      "195 0.57011783 [[-1.2661151  -0.6789886   0.9811025 ]\n",
      " [ 0.44218704  0.24401174  0.17412375]\n",
      " [ 0.48033535  0.20355335 -0.62319905]]\n",
      "196 0.5694895 [[-1.2697223  -0.6793231   0.98504424]\n",
      " [ 0.44204238  0.2442243   0.17405586]\n",
      " [ 0.48190793  0.20363176 -0.62485003]]\n",
      "197 0.56886435 [[-1.2733246  -0.67965066  0.98897415]\n",
      " [ 0.44190198  0.24443758  0.173983  ]\n",
      " [ 0.4834739   0.20370655 -0.6264908 ]]\n",
      "198 0.5682423 [[-1.276922   -0.67997146  0.9928924 ]\n",
      " [ 0.4417658   0.24465144  0.17390533]\n",
      " [ 0.48503342  0.20377772 -0.6281215 ]]\n",
      "199 0.5676235 [[-1.2805145  -0.6802855   0.996799  ]\n",
      " [ 0.44163364  0.24486588  0.17382303]\n",
      " [ 0.48658657  0.20384543 -0.6297424 ]]\n",
      "200 0.5670078 [[-1.2841021  -0.68059295  1.000694  ]\n",
      " [ 0.44150552  0.2450808   0.17373623]\n",
      " [ 0.48813352  0.20390971 -0.6313536 ]]\n",
      "Prediction:  [2 2 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = [[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5], [1, 7, 5], [1, 2, 5], [1, 6, 6], [1, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1], [3, 1, 2], [3, 3, 4]]\n",
    "y_test = [[0, 0, 1], [0, 0, 1], [0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "    \n",
    "    # predict\n",
    "    print(\"Prediction: \", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a831318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "1 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "2 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "3 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "4 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "5 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "6 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "7 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "8 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "9 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "10 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "11 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "12 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "13 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "14 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "15 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "16 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "17 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "18 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "19 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "20 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "21 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "22 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "23 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "24 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "25 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "26 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "27 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "28 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "29 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "30 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "31 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "32 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "33 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "34 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "35 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "36 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "37 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "38 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "39 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "40 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "41 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "42 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "43 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "44 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "45 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "46 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "47 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "48 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "49 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "50 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "51 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "52 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "53 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "54 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "55 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "56 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "57 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "58 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "59 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "60 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "61 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "62 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "63 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "64 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "65 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "66 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "67 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "68 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "69 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "70 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "71 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "72 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "73 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "74 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "75 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "76 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "77 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "78 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "79 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "80 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "81 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "82 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "83 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "84 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "85 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "86 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "87 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "88 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "89 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "90 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "91 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "92 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "93 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "94 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "95 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "96 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "97 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "98 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "99 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "100 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "101 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "102 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "103 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "104 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "105 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "106 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "107 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "108 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "109 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "110 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "111 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "112 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "113 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "114 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "115 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "116 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "117 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "118 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "119 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "120 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "121 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "122 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "123 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "124 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "125 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "126 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "127 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "128 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "129 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "130 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "131 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "132 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "133 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "134 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "135 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "136 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "137 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "138 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "139 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "140 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "141 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "142 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "143 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "144 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "145 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "146 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "147 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "148 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "149 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "150 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "151 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "152 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "153 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "154 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "155 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "156 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "157 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "158 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "159 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "160 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "161 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "162 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "163 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "164 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "165 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "166 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "167 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "168 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "169 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "170 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "171 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "172 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "173 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "174 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "175 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "176 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "177 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "178 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "179 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "180 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "181 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "182 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "183 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "184 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "185 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "186 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "187 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "188 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "189 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "190 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "191 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "192 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "193 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "194 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "195 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "196 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "197 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "198 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "199 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "200 4.242146 [[ 0.09913177  0.3872115   0.15639298]\n",
      " [ 0.21949506  1.2800403   0.7673686 ]\n",
      " [-0.46814746  0.5079432  -0.62223613]]\n",
      "Prediction:  [1 1 1]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = [[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5], [1, 7, 5], [1, 2, 5], [1, 6, 6], [1, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1], [3, 1, 2], [3, 3, 4]]\n",
    "y_test = [[0, 0, 1], [0, 0, 1], [0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-10).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "    \n",
    "    # predict\n",
    "    print(\"Prediction: \", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5941f6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  4318802700000.0 \n",
      "Prediction:\n",
      " [[1466487.1]\n",
      " [2952215. ]\n",
      " [2322395.2]\n",
      " [1627979.9]\n",
      " [1918664.1]\n",
      " [1934813.1]\n",
      " [1773321.1]\n",
      " [2257800.2]]\n",
      "1 Cost:  4.7449843e+27 \n",
      "Prediction:\n",
      " [[-4.8590202e+13]\n",
      " [-9.7817058e+13]\n",
      " [-7.6949154e+13]\n",
      " [-5.3940943e+13]\n",
      " [-6.3572281e+13]\n",
      " [-6.4107357e+13]\n",
      " [-5.8756612e+13]\n",
      " [-7.4808843e+13]]\n",
      "2 Cost:  inf \n",
      "Prediction:\n",
      " [[1.6105877e+21]\n",
      " [3.2422782e+21]\n",
      " [2.5505832e+21]\n",
      " [1.7879455e+21]\n",
      " [2.1071892e+21]\n",
      " [2.1249251e+21]\n",
      " [1.9475673e+21]\n",
      " [2.4796403e+21]]\n",
      "3 Cost:  inf \n",
      "Prediction:\n",
      " [[-5.3385106e+28]\n",
      " [-1.0746969e+29]\n",
      " [-8.4542534e+28]\n",
      " [-5.9263867e+28]\n",
      " [-6.9845637e+28]\n",
      " [-7.0433511e+28]\n",
      " [-6.4554750e+28]\n",
      " [-8.2191022e+28]]\n",
      "4 Cost:  inf \n",
      "Prediction:\n",
      " [[1.7695214e+36]\n",
      " [3.5622281e+36]\n",
      " [2.8022763e+36]\n",
      " [1.9643808e+36]\n",
      " [2.3151279e+36]\n",
      " [2.3346137e+36]\n",
      " [2.1397541e+36]\n",
      " [2.7243323e+36]]\n",
      "5 Cost:  inf \n",
      "Prediction:\n",
      " [[-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]]\n",
      "6 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "7 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "8 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "9 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "10 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "11 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "12 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "13 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "14 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "15 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "16 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "17 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "18 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "19 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "20 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "21 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "22 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "23 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "24 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "25 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "26 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "27 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "28 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "29 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "30 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "31 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "32 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "33 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "34 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "35 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "36 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "37 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "38 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "39 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "40 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "41 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "42 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "43 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "44 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "45 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "46 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "47 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "48 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "49 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "50 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "51 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "52 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "53 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "54 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "55 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "56 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "57 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "58 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "59 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "60 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "61 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "62 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "63 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "64 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "65 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "66 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "67 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "68 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "69 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "70 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "71 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "72 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "73 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "74 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "75 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "76 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "77 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "78 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "79 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "80 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "81 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "82 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "83 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "84 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "85 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "86 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "87 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "88 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "89 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "90 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "91 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "92 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "93 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "94 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "95 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "96 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "97 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "98 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "99 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "100 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd9bfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
      " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
      "0 Cost:  0.34438485 \n",
      "Prediction:\n",
      " [[-0.05683047]\n",
      " [ 0.5287802 ]\n",
      " [ 0.5673416 ]\n",
      " [ 0.6226775 ]\n",
      " [ 0.5227794 ]\n",
      " [ 0.51509374]\n",
      " [ 0.87443906]\n",
      " [ 1.0080848 ]]\n",
      "1 Cost:  0.34438235 \n",
      "Prediction:\n",
      " [[-0.05682361]\n",
      " [ 0.52878374]\n",
      " [ 0.5673443 ]\n",
      " [ 0.62267905]\n",
      " [ 0.52278197]\n",
      " [ 0.51509595]\n",
      " [ 0.8744384 ]\n",
      " [ 1.0080831 ]]\n",
      "2 Cost:  0.34437978 \n",
      "Prediction:\n",
      " [[-0.0568167 ]\n",
      " [ 0.5287874 ]\n",
      " [ 0.5673469 ]\n",
      " [ 0.6226806 ]\n",
      " [ 0.5227844 ]\n",
      " [ 0.5150981 ]\n",
      " [ 0.87443775]\n",
      " [ 1.0080814 ]]\n",
      "3 Cost:  0.34437728 \n",
      "Prediction:\n",
      " [[-0.0568099 ]\n",
      " [ 0.5287909 ]\n",
      " [ 0.5673495 ]\n",
      " [ 0.62268215]\n",
      " [ 0.5227869 ]\n",
      " [ 0.5151003 ]\n",
      " [ 0.8744371 ]\n",
      " [ 1.0080798 ]]\n",
      "4 Cost:  0.34437475 \n",
      "Prediction:\n",
      " [[-0.05680305]\n",
      " [ 0.52879447]\n",
      " [ 0.56735206]\n",
      " [ 0.6226837 ]\n",
      " [ 0.5227895 ]\n",
      " [ 0.5151025 ]\n",
      " [ 0.8744364 ]\n",
      " [ 1.0080782 ]]\n",
      "5 Cost:  0.3443722 \n",
      "Prediction:\n",
      " [[-0.05679619]\n",
      " [ 0.5287979 ]\n",
      " [ 0.5673547 ]\n",
      " [ 0.6226853 ]\n",
      " [ 0.522792  ]\n",
      " [ 0.51510465]\n",
      " [ 0.8744357 ]\n",
      " [ 1.0080765 ]]\n",
      "6 Cost:  0.3443697 \n",
      "Prediction:\n",
      " [[-0.05678928]\n",
      " [ 0.52880144]\n",
      " [ 0.56735736]\n",
      " [ 0.62268686]\n",
      " [ 0.5227945 ]\n",
      " [ 0.5151068 ]\n",
      " [ 0.87443507]\n",
      " [ 1.0080749 ]]\n",
      "7 Cost:  0.34436718 \n",
      "Prediction:\n",
      " [[-0.05678248]\n",
      " [ 0.528805  ]\n",
      " [ 0.5673599 ]\n",
      " [ 0.6226884 ]\n",
      " [ 0.522797  ]\n",
      " [ 0.515109  ]\n",
      " [ 0.8744344 ]\n",
      " [ 1.0080732 ]]\n",
      "8 Cost:  0.34436464 \n",
      "Prediction:\n",
      " [[-0.05677563]\n",
      " [ 0.5288086 ]\n",
      " [ 0.56736255]\n",
      " [ 0.62268996]\n",
      " [ 0.5227995 ]\n",
      " [ 0.51511115]\n",
      " [ 0.8744337 ]\n",
      " [ 1.0080717 ]]\n",
      "9 Cost:  0.34436214 \n",
      "Prediction:\n",
      " [[-0.05676877]\n",
      " [ 0.5288121 ]\n",
      " [ 0.56736517]\n",
      " [ 0.6226915 ]\n",
      " [ 0.522802  ]\n",
      " [ 0.51511335]\n",
      " [ 0.87443304]\n",
      " [ 1.00807   ]]\n",
      "10 Cost:  0.34435958 \n",
      "Prediction:\n",
      " [[-0.05676186]\n",
      " [ 0.5288156 ]\n",
      " [ 0.5673678 ]\n",
      " [ 0.62269306]\n",
      " [ 0.5228045 ]\n",
      " [ 0.5151155 ]\n",
      " [ 0.8744324 ]\n",
      " [ 1.0080683 ]]\n",
      "11 Cost:  0.34435704 \n",
      "Prediction:\n",
      " [[-0.05675507]\n",
      " [ 0.5288192 ]\n",
      " [ 0.56737036]\n",
      " [ 0.6226946 ]\n",
      " [ 0.522807  ]\n",
      " [ 0.51511776]\n",
      " [ 0.8744317 ]\n",
      " [ 1.0080667 ]]\n",
      "12 Cost:  0.34435448 \n",
      "Prediction:\n",
      " [[-0.05674821]\n",
      " [ 0.5288227 ]\n",
      " [ 0.56737304]\n",
      " [ 0.62269616]\n",
      " [ 0.52280957]\n",
      " [ 0.5151199 ]\n",
      " [ 0.874431  ]\n",
      " [ 1.008065  ]]\n",
      "13 Cost:  0.344352 \n",
      "Prediction:\n",
      " [[-0.05674136]\n",
      " [ 0.52882624]\n",
      " [ 0.56737566]\n",
      " [ 0.6226977 ]\n",
      " [ 0.52281207]\n",
      " [ 0.51512206]\n",
      " [ 0.87443036]\n",
      " [ 1.0080634 ]]\n",
      "14 Cost:  0.34434944 \n",
      "Prediction:\n",
      " [[-0.05673444]\n",
      " [ 0.5288298 ]\n",
      " [ 0.5673782 ]\n",
      " [ 0.6226993 ]\n",
      " [ 0.52281463]\n",
      " [ 0.5151243 ]\n",
      " [ 0.8744297 ]\n",
      " [ 1.0080618 ]]\n",
      "15 Cost:  0.3443469 \n",
      "Prediction:\n",
      " [[-0.05672765]\n",
      " [ 0.5288334 ]\n",
      " [ 0.5673809 ]\n",
      " [ 0.62270087]\n",
      " [ 0.5228171 ]\n",
      " [ 0.51512647]\n",
      " [ 0.874429  ]\n",
      " [ 1.0080601 ]]\n",
      "16 Cost:  0.34434438 \n",
      "Prediction:\n",
      " [[-0.05672079]\n",
      " [ 0.52883685]\n",
      " [ 0.5673835 ]\n",
      " [ 0.6227024 ]\n",
      " [ 0.5228195 ]\n",
      " [ 0.5151287 ]\n",
      " [ 0.87442833]\n",
      " [ 1.0080584 ]]\n",
      "17 Cost:  0.34434187 \n",
      "Prediction:\n",
      " [[-0.05671394]\n",
      " [ 0.5288404 ]\n",
      " [ 0.56738615]\n",
      " [ 0.62270397]\n",
      " [ 0.522822  ]\n",
      " [ 0.5151308 ]\n",
      " [ 0.8744277 ]\n",
      " [ 1.0080569 ]]\n",
      "18 Cost:  0.3443393 \n",
      "Prediction:\n",
      " [[-0.05670702]\n",
      " [ 0.52884394]\n",
      " [ 0.56738865]\n",
      " [ 0.6227055 ]\n",
      " [ 0.52282465]\n",
      " [ 0.515133  ]\n",
      " [ 0.874427  ]\n",
      " [ 1.0080552 ]]\n",
      "19 Cost:  0.3443368 \n",
      "Prediction:\n",
      " [[-0.05670023]\n",
      " [ 0.52884746]\n",
      " [ 0.56739134]\n",
      " [ 0.6227071 ]\n",
      " [ 0.52282715]\n",
      " [ 0.51513517]\n",
      " [ 0.8744263 ]\n",
      " [ 1.0080535 ]]\n",
      "20 Cost:  0.34433424 \n",
      "Prediction:\n",
      " [[-0.05669338]\n",
      " [ 0.528851  ]\n",
      " [ 0.56739396]\n",
      " [ 0.6227087 ]\n",
      " [ 0.5228296 ]\n",
      " [ 0.5151374 ]\n",
      " [ 0.87442565]\n",
      " [ 1.0080519 ]]\n",
      "21 Cost:  0.34433174 \n",
      "Prediction:\n",
      " [[-0.05668652]\n",
      " [ 0.5288546 ]\n",
      " [ 0.5673965 ]\n",
      " [ 0.6227102 ]\n",
      " [ 0.5228321 ]\n",
      " [ 0.5151395 ]\n",
      " [ 0.874425  ]\n",
      " [ 1.0080502 ]]\n",
      "22 Cost:  0.34432918 \n",
      "Prediction:\n",
      " [[-0.05667961]\n",
      " [ 0.5288582 ]\n",
      " [ 0.5673992 ]\n",
      " [ 0.6227118 ]\n",
      " [ 0.5228346 ]\n",
      " [ 0.51514167]\n",
      " [ 0.87442434]\n",
      " [ 1.0080485 ]]\n",
      "23 Cost:  0.3443267 \n",
      "Prediction:\n",
      " [[-0.05667281]\n",
      " [ 0.52886164]\n",
      " [ 0.56740177]\n",
      " [ 0.6227133 ]\n",
      " [ 0.52283716]\n",
      " [ 0.5151439 ]\n",
      " [ 0.8744236 ]\n",
      " [ 1.008047  ]]\n",
      "24 Cost:  0.34432414 \n",
      "Prediction:\n",
      " [[-0.05666596]\n",
      " [ 0.5288652 ]\n",
      " [ 0.56740445]\n",
      " [ 0.6227149 ]\n",
      " [ 0.52283967]\n",
      " [ 0.5151461 ]\n",
      " [ 0.87442297]\n",
      " [ 1.0080453 ]]\n",
      "25 Cost:  0.34432164 \n",
      "Prediction:\n",
      " [[-0.0566591 ]\n",
      " [ 0.5288687 ]\n",
      " [ 0.567407  ]\n",
      " [ 0.6227164 ]\n",
      " [ 0.52284217]\n",
      " [ 0.5151482 ]\n",
      " [ 0.8744223 ]\n",
      " [ 1.0080436 ]]\n",
      "26 Cost:  0.3443191 \n",
      "Prediction:\n",
      " [[-0.05665219]\n",
      " [ 0.5288723 ]\n",
      " [ 0.5674097 ]\n",
      " [ 0.622718  ]\n",
      " [ 0.5228447 ]\n",
      " [ 0.5151504 ]\n",
      " [ 0.87442166]\n",
      " [ 1.008042  ]]\n",
      "27 Cost:  0.3443166 \n",
      "Prediction:\n",
      " [[-0.05664539]\n",
      " [ 0.5288758 ]\n",
      " [ 0.56741226]\n",
      " [ 0.6227196 ]\n",
      " [ 0.5228472 ]\n",
      " [ 0.5151526 ]\n",
      " [ 0.87442094]\n",
      " [ 1.0080404 ]]\n",
      "28 Cost:  0.34431404 \n",
      "Prediction:\n",
      " [[-0.05663854]\n",
      " [ 0.5288794 ]\n",
      " [ 0.5674149 ]\n",
      " [ 0.62272114]\n",
      " [ 0.5228497 ]\n",
      " [ 0.51515484]\n",
      " [ 0.8744203 ]\n",
      " [ 1.0080388 ]]\n",
      "29 Cost:  0.34431154 \n",
      "Prediction:\n",
      " [[-0.05663168]\n",
      " [ 0.5288829 ]\n",
      " [ 0.5674175 ]\n",
      " [ 0.6227227 ]\n",
      " [ 0.52285224]\n",
      " [ 0.515157  ]\n",
      " [ 0.8744196 ]\n",
      " [ 1.0080371 ]]\n",
      "30 Cost:  0.34430897 \n",
      "Prediction:\n",
      " [[-0.05662477]\n",
      " [ 0.52888644]\n",
      " [ 0.5674201 ]\n",
      " [ 0.62272424]\n",
      " [ 0.52285475]\n",
      " [ 0.51515913]\n",
      " [ 0.874419  ]\n",
      " [ 1.0080354 ]]\n",
      "31 Cost:  0.3443064 \n",
      "Prediction:\n",
      " [[-0.05661798]\n",
      " [ 0.52888995]\n",
      " [ 0.56742275]\n",
      " [ 0.6227258 ]\n",
      " [ 0.52285725]\n",
      " [ 0.5151614 ]\n",
      " [ 0.87441826]\n",
      " [ 1.0080338 ]]\n",
      "32 Cost:  0.3443039 \n",
      "Prediction:\n",
      " [[-0.05661112]\n",
      " [ 0.5288935 ]\n",
      " [ 0.5674254 ]\n",
      " [ 0.6227274 ]\n",
      " [ 0.5228598 ]\n",
      " [ 0.5151635 ]\n",
      " [ 0.8744176 ]\n",
      " [ 1.0080322 ]]\n",
      "33 Cost:  0.3443014 \n",
      "Prediction:\n",
      " [[-0.05660427]\n",
      " [ 0.52889705]\n",
      " [ 0.567428  ]\n",
      " [ 0.62272894]\n",
      " [ 0.52286226]\n",
      " [ 0.5151657 ]\n",
      " [ 0.87441695]\n",
      " [ 1.0080305 ]]\n",
      "34 Cost:  0.34429884 \n",
      "Prediction:\n",
      " [[-0.05659735]\n",
      " [ 0.52890056]\n",
      " [ 0.5674306 ]\n",
      " [ 0.6227305 ]\n",
      " [ 0.5228647 ]\n",
      " [ 0.5151679 ]\n",
      " [ 0.8744163 ]\n",
      " [ 1.0080289 ]]\n",
      "35 Cost:  0.34429634 \n",
      "Prediction:\n",
      " [[-0.05659056]\n",
      " [ 0.52890414]\n",
      " [ 0.56743324]\n",
      " [ 0.62273204]\n",
      " [ 0.52286726]\n",
      " [ 0.51517004]\n",
      " [ 0.87441564]\n",
      " [ 1.0080272 ]]\n",
      "36 Cost:  0.3442938 \n",
      "Prediction:\n",
      " [[-0.0565837 ]\n",
      " [ 0.52890766]\n",
      " [ 0.5674358 ]\n",
      " [ 0.6227336 ]\n",
      " [ 0.5228698 ]\n",
      " [ 0.51517224]\n",
      " [ 0.8744149 ]\n",
      " [ 1.0080256 ]]\n",
      "37 Cost:  0.34429127 \n",
      "Prediction:\n",
      " [[-0.05657685]\n",
      " [ 0.52891123]\n",
      " [ 0.56743836]\n",
      " [ 0.62273514]\n",
      " [ 0.5228723 ]\n",
      " [ 0.5151744 ]\n",
      " [ 0.87441427]\n",
      " [ 1.008024  ]]\n",
      "38 Cost:  0.34428877 \n",
      "Prediction:\n",
      " [[-0.05656993]\n",
      " [ 0.5289147 ]\n",
      " [ 0.56744105]\n",
      " [ 0.6227367 ]\n",
      " [ 0.52287483]\n",
      " [ 0.5151766 ]\n",
      " [ 0.8744136 ]\n",
      " [ 1.0080223 ]]\n",
      "39 Cost:  0.3442862 \n",
      "Prediction:\n",
      " [[-0.05656314]\n",
      " [ 0.52891827]\n",
      " [ 0.5674436 ]\n",
      " [ 0.62273824]\n",
      " [ 0.5228772 ]\n",
      " [ 0.51517874]\n",
      " [ 0.8744129 ]\n",
      " [ 1.0080206 ]]\n",
      "40 Cost:  0.3442837 \n",
      "Prediction:\n",
      " [[-0.05655634]\n",
      " [ 0.52892184]\n",
      " [ 0.5674463 ]\n",
      " [ 0.6227398 ]\n",
      " [ 0.52287984]\n",
      " [ 0.5151809 ]\n",
      " [ 0.87441224]\n",
      " [ 1.008019  ]]\n",
      "41 Cost:  0.3442812 \n",
      "Prediction:\n",
      " [[-0.05654955]\n",
      " [ 0.5289253 ]\n",
      " [ 0.5674489 ]\n",
      " [ 0.62274134]\n",
      " [ 0.52288234]\n",
      " [ 0.51518303]\n",
      " [ 0.8744116 ]\n",
      " [ 1.0080174 ]]\n",
      "42 Cost:  0.34427863 \n",
      "Prediction:\n",
      " [[-0.05654263]\n",
      " [ 0.5289289 ]\n",
      " [ 0.5674515 ]\n",
      " [ 0.6227429 ]\n",
      " [ 0.5228848 ]\n",
      " [ 0.51518524]\n",
      " [ 0.87441087]\n",
      " [ 1.0080158 ]]\n",
      "43 Cost:  0.34427613 \n",
      "Prediction:\n",
      " [[-0.05653584]\n",
      " [ 0.52893233]\n",
      " [ 0.5674541 ]\n",
      " [ 0.62274444]\n",
      " [ 0.5228873 ]\n",
      " [ 0.51518744]\n",
      " [ 0.8744102 ]\n",
      " [ 1.0080141 ]]\n",
      "44 Cost:  0.34427363 \n",
      "Prediction:\n",
      " [[-0.05652905]\n",
      " [ 0.5289359 ]\n",
      " [ 0.5674566 ]\n",
      " [ 0.622746  ]\n",
      " [ 0.5228898 ]\n",
      " [ 0.5151896 ]\n",
      " [ 0.87440956]\n",
      " [ 1.0080124 ]]\n",
      "45 Cost:  0.34427112 \n",
      "Prediction:\n",
      " [[-0.05652225]\n",
      " [ 0.52893937]\n",
      " [ 0.5674592 ]\n",
      " [ 0.62274754]\n",
      " [ 0.52289224]\n",
      " [ 0.51519173]\n",
      " [ 0.87440884]\n",
      " [ 1.0080109 ]]\n",
      "46 Cost:  0.34426856 \n",
      "Prediction:\n",
      " [[-0.05651534]\n",
      " [ 0.5289429 ]\n",
      " [ 0.56746185]\n",
      " [ 0.6227491 ]\n",
      " [ 0.52289486]\n",
      " [ 0.51519394]\n",
      " [ 0.8744082 ]\n",
      " [ 1.0080092 ]]\n",
      "47 Cost:  0.34426606 \n",
      "Prediction:\n",
      " [[-0.05650854]\n",
      " [ 0.5289464 ]\n",
      " [ 0.5674645 ]\n",
      " [ 0.6227506 ]\n",
      " [ 0.5228973 ]\n",
      " [ 0.5151961 ]\n",
      " [ 0.8744075 ]\n",
      " [ 1.0080075 ]]\n",
      "48 Cost:  0.34426352 \n",
      "Prediction:\n",
      " [[-0.05650175]\n",
      " [ 0.5289499 ]\n",
      " [ 0.5674671 ]\n",
      " [ 0.62275213]\n",
      " [ 0.52289975]\n",
      " [ 0.51519823]\n",
      " [ 0.8744068 ]\n",
      " [ 1.0080059 ]]\n",
      "49 Cost:  0.344261 \n",
      "Prediction:\n",
      " [[-0.05649495]\n",
      " [ 0.52895343]\n",
      " [ 0.56746966]\n",
      " [ 0.62275374]\n",
      " [ 0.52290225]\n",
      " [ 0.5152004 ]\n",
      " [ 0.87440616]\n",
      " [ 1.0080042 ]]\n",
      "50 Cost:  0.34425846 \n",
      "Prediction:\n",
      " [[-0.05648804]\n",
      " [ 0.52895695]\n",
      " [ 0.5674723 ]\n",
      " [ 0.6227553 ]\n",
      " [ 0.52290475]\n",
      " [ 0.5152025 ]\n",
      " [ 0.87440544]\n",
      " [ 1.0080025 ]]\n",
      "51 Cost:  0.34425595 \n",
      "Prediction:\n",
      " [[-0.05648124]\n",
      " [ 0.52896047]\n",
      " [ 0.56747484]\n",
      " [ 0.6227568 ]\n",
      " [ 0.52290726]\n",
      " [ 0.5152048 ]\n",
      " [ 0.8744048 ]\n",
      " [ 1.008001  ]]\n",
      "52 Cost:  0.34425345 \n",
      "Prediction:\n",
      " [[-0.05647445]\n",
      " [ 0.528964  ]\n",
      " [ 0.5674774 ]\n",
      " [ 0.6227583 ]\n",
      " [ 0.52290976]\n",
      " [ 0.5152069 ]\n",
      " [ 0.87440413]\n",
      " [ 1.0079993 ]]\n",
      "53 Cost:  0.34425092 \n",
      "Prediction:\n",
      " [[-0.05646765]\n",
      " [ 0.52896756]\n",
      " [ 0.56747997]\n",
      " [ 0.6227599 ]\n",
      " [ 0.5229123 ]\n",
      " [ 0.5152091 ]\n",
      " [ 0.8744034 ]\n",
      " [ 1.0079976 ]]\n",
      "54 Cost:  0.34424835 \n",
      "Prediction:\n",
      " [[-0.05646074]\n",
      " [ 0.528971  ]\n",
      " [ 0.5674826 ]\n",
      " [ 0.6227614 ]\n",
      " [ 0.52291477]\n",
      " [ 0.5152112 ]\n",
      " [ 0.87440276]\n",
      " [ 1.007996  ]]\n",
      "55 Cost:  0.34424585 \n",
      "Prediction:\n",
      " [[-0.05645394]\n",
      " [ 0.52897453]\n",
      " [ 0.5674853 ]\n",
      " [ 0.622763  ]\n",
      " [ 0.5229172 ]\n",
      " [ 0.51521343]\n",
      " [ 0.87440205]\n",
      " [ 1.0079944 ]]\n",
      "56 Cost:  0.34424335 \n",
      "Prediction:\n",
      " [[-0.05644715]\n",
      " [ 0.5289781 ]\n",
      " [ 0.56748784]\n",
      " [ 0.6227645 ]\n",
      " [ 0.52291965]\n",
      " [ 0.5152155 ]\n",
      " [ 0.8744014 ]\n",
      " [ 1.0079927 ]]\n",
      "57 Cost:  0.34424084 \n",
      "Prediction:\n",
      " [[-0.05644035]\n",
      " [ 0.52898157]\n",
      " [ 0.56749046]\n",
      " [ 0.6227661 ]\n",
      " [ 0.52292216]\n",
      " [ 0.5152177 ]\n",
      " [ 0.87440073]\n",
      " [ 1.0079911 ]]\n",
      "58 Cost:  0.34423828 \n",
      "Prediction:\n",
      " [[-0.05643344]\n",
      " [ 0.528985  ]\n",
      " [ 0.5674931 ]\n",
      " [ 0.62276757]\n",
      " [ 0.5229248 ]\n",
      " [ 0.51521987]\n",
      " [ 0.8744    ]\n",
      " [ 1.0079894 ]]\n",
      "59 Cost:  0.34423578 \n",
      "Prediction:\n",
      " [[-0.05642664]\n",
      " [ 0.52898866]\n",
      " [ 0.5674957 ]\n",
      " [ 0.6227691 ]\n",
      " [ 0.5229272 ]\n",
      " [ 0.5152221 ]\n",
      " [ 0.87439936]\n",
      " [ 1.0079877 ]]\n",
      "60 Cost:  0.3442333 \n",
      "Prediction:\n",
      " [[-0.05641985]\n",
      " [ 0.5289922 ]\n",
      " [ 0.56749827]\n",
      " [ 0.6227707 ]\n",
      " [ 0.52292967]\n",
      " [ 0.5152242 ]\n",
      " [ 0.8743987 ]\n",
      " [ 1.0079862 ]]\n",
      "61 Cost:  0.34423077 \n",
      "Prediction:\n",
      " [[-0.05641305]\n",
      " [ 0.5289957 ]\n",
      " [ 0.5675008 ]\n",
      " [ 0.6227722 ]\n",
      " [ 0.5229322 ]\n",
      " [ 0.51522636]\n",
      " [ 0.874398  ]\n",
      " [ 1.0079845 ]]\n",
      "62 Cost:  0.3442282 \n",
      "Prediction:\n",
      " [[-0.05640614]\n",
      " [ 0.5289992 ]\n",
      " [ 0.56750345]\n",
      " [ 0.6227738 ]\n",
      " [ 0.5229347 ]\n",
      " [ 0.51522857]\n",
      " [ 0.87439734]\n",
      " [ 1.0079829 ]]\n",
      "63 Cost:  0.3442257 \n",
      "Prediction:\n",
      " [[-0.05639935]\n",
      " [ 0.5290027 ]\n",
      " [ 0.567506  ]\n",
      " [ 0.6227753 ]\n",
      " [ 0.5229371 ]\n",
      " [ 0.5152307 ]\n",
      " [ 0.8743967 ]\n",
      " [ 1.0079812 ]]\n",
      "64 Cost:  0.3442232 \n",
      "Prediction:\n",
      " [[-0.05639255]\n",
      " [ 0.52900624]\n",
      " [ 0.56750864]\n",
      " [ 0.62277687]\n",
      " [ 0.5229397 ]\n",
      " [ 0.51523286]\n",
      " [ 0.87439597]\n",
      " [ 1.0079796 ]]\n",
      "65 Cost:  0.34422067 \n",
      "Prediction:\n",
      " [[-0.05638576]\n",
      " [ 0.5290098 ]\n",
      " [ 0.5675112 ]\n",
      " [ 0.6227784 ]\n",
      " [ 0.5229422 ]\n",
      " [ 0.51523507]\n",
      " [ 0.8743953 ]\n",
      " [ 1.007978  ]]\n",
      "66 Cost:  0.34421813 \n",
      "Prediction:\n",
      " [[-0.05637884]\n",
      " [ 0.5290133 ]\n",
      " [ 0.5675138 ]\n",
      " [ 0.62277997]\n",
      " [ 0.5229446 ]\n",
      " [ 0.5152372 ]\n",
      " [ 0.87439466]\n",
      " [ 1.0079763 ]]\n",
      "67 Cost:  0.3442156 \n",
      "Prediction:\n",
      " [[-0.05637205]\n",
      " [ 0.52901673]\n",
      " [ 0.56751645]\n",
      " [ 0.6227815 ]\n",
      " [ 0.52294713]\n",
      " [ 0.51523936]\n",
      " [ 0.87439394]\n",
      " [ 1.0079746 ]]\n",
      "68 Cost:  0.3442131 \n",
      "Prediction:\n",
      " [[-0.05636525]\n",
      " [ 0.52902025]\n",
      " [ 0.567519  ]\n",
      " [ 0.62278306]\n",
      " [ 0.52294964]\n",
      " [ 0.5152415 ]\n",
      " [ 0.8743933 ]\n",
      " [ 1.007973  ]]\n",
      "69 Cost:  0.34421057 \n",
      "Prediction:\n",
      " [[-0.05635846]\n",
      " [ 0.5290238 ]\n",
      " [ 0.5675217 ]\n",
      " [ 0.6227846 ]\n",
      " [ 0.52295214]\n",
      " [ 0.5152437 ]\n",
      " [ 0.87439257]\n",
      " [ 1.0079713 ]]\n",
      "70 Cost:  0.34420806 \n",
      "Prediction:\n",
      " [[-0.05635154]\n",
      " [ 0.5290273 ]\n",
      " [ 0.5675242 ]\n",
      " [ 0.62278616]\n",
      " [ 0.5229546 ]\n",
      " [ 0.51524585]\n",
      " [ 0.8743919 ]\n",
      " [ 1.0079697 ]]\n",
      "71 Cost:  0.34420556 \n",
      "Prediction:\n",
      " [[-0.05634475]\n",
      " [ 0.5290308 ]\n",
      " [ 0.5675268 ]\n",
      " [ 0.6227877 ]\n",
      " [ 0.5229572 ]\n",
      " [ 0.51524806]\n",
      " [ 0.87439126]\n",
      " [ 1.0079681 ]]\n",
      "72 Cost:  0.344203 \n",
      "Prediction:\n",
      " [[-0.05633795]\n",
      " [ 0.52903444]\n",
      " [ 0.56752944]\n",
      " [ 0.62278926]\n",
      " [ 0.52295965]\n",
      " [ 0.5152502 ]\n",
      " [ 0.87439054]\n",
      " [ 1.0079664 ]]\n",
      "73 Cost:  0.3442005 \n",
      "Prediction:\n",
      " [[-0.05633116]\n",
      " [ 0.5290379 ]\n",
      " [ 0.56753206]\n",
      " [ 0.62279075]\n",
      " [ 0.5229621 ]\n",
      " [ 0.51525235]\n",
      " [ 0.8743899 ]\n",
      " [ 1.0079647 ]]\n",
      "74 Cost:  0.34419796 \n",
      "Prediction:\n",
      " [[-0.05632424]\n",
      " [ 0.52904147]\n",
      " [ 0.5675347 ]\n",
      " [ 0.6227923 ]\n",
      " [ 0.52296454]\n",
      " [ 0.51525456]\n",
      " [ 0.87438923]\n",
      " [ 1.0079632 ]]\n",
      "75 Cost:  0.34419546 \n",
      "Prediction:\n",
      " [[-0.05631745]\n",
      " [ 0.5290449 ]\n",
      " [ 0.56753725]\n",
      " [ 0.62279385]\n",
      " [ 0.5229671 ]\n",
      " [ 0.5152567 ]\n",
      " [ 0.8743885 ]\n",
      " [ 1.0079615 ]]\n",
      "76 Cost:  0.34419292 \n",
      "Prediction:\n",
      " [[-0.05631065]\n",
      " [ 0.5290485 ]\n",
      " [ 0.5675399 ]\n",
      " [ 0.62279546]\n",
      " [ 0.5229696 ]\n",
      " [ 0.5152589 ]\n",
      " [ 0.87438786]\n",
      " [ 1.0079598 ]]\n",
      "77 Cost:  0.34419042 \n",
      "Prediction:\n",
      " [[-0.05630386]\n",
      " [ 0.5290519 ]\n",
      " [ 0.5675424 ]\n",
      " [ 0.62279695]\n",
      " [ 0.5229721 ]\n",
      " [ 0.515261  ]\n",
      " [ 0.8743872 ]\n",
      " [ 1.0079582 ]]\n",
      "78 Cost:  0.34418786 \n",
      "Prediction:\n",
      " [[-0.05629694]\n",
      " [ 0.5290555 ]\n",
      " [ 0.567545  ]\n",
      " [ 0.6227985 ]\n",
      " [ 0.5229746 ]\n",
      " [ 0.5152632 ]\n",
      " [ 0.8743865 ]\n",
      " [ 1.0079565 ]]\n",
      "79 Cost:  0.34418535 \n",
      "Prediction:\n",
      " [[-0.05629015]\n",
      " [ 0.529059  ]\n",
      " [ 0.56754756]\n",
      " [ 0.62280005]\n",
      " [ 0.5229771 ]\n",
      " [ 0.5152654 ]\n",
      " [ 0.87438583]\n",
      " [ 1.007955  ]]\n",
      "80 Cost:  0.34418285 \n",
      "Prediction:\n",
      " [[-0.05628335]\n",
      " [ 0.5290625 ]\n",
      " [ 0.5675502 ]\n",
      " [ 0.6228016 ]\n",
      " [ 0.52297956]\n",
      " [ 0.51526755]\n",
      " [ 0.8743851 ]\n",
      " [ 1.0079533 ]]\n",
      "81 Cost:  0.34418035 \n",
      "Prediction:\n",
      " [[-0.05627656]\n",
      " [ 0.529066  ]\n",
      " [ 0.5675528 ]\n",
      " [ 0.62280315]\n",
      " [ 0.52298206]\n",
      " [ 0.5152697 ]\n",
      " [ 0.87438446]\n",
      " [ 1.0079516 ]]\n",
      "82 Cost:  0.34417778 \n",
      "Prediction:\n",
      " [[-0.05626965]\n",
      " [ 0.52906954]\n",
      " [ 0.5675554 ]\n",
      " [ 0.62280464]\n",
      " [ 0.52298456]\n",
      " [ 0.5152719 ]\n",
      " [ 0.8743838 ]\n",
      " [ 1.00795   ]]\n",
      "83 Cost:  0.34417528 \n",
      "Prediction:\n",
      " [[-0.05626285]\n",
      " [ 0.52907306]\n",
      " [ 0.56755805]\n",
      " [ 0.6228062 ]\n",
      " [ 0.522987  ]\n",
      " [ 0.515274  ]\n",
      " [ 0.8743831 ]\n",
      " [ 1.0079484 ]]\n",
      "84 Cost:  0.34417278 \n",
      "Prediction:\n",
      " [[-0.05625606]\n",
      " [ 0.5290766 ]\n",
      " [ 0.5675606 ]\n",
      " [ 0.6228078 ]\n",
      " [ 0.5229895 ]\n",
      " [ 0.5152762 ]\n",
      " [ 0.87438244]\n",
      " [ 1.0079467 ]]\n",
      "85 Cost:  0.34417027 \n",
      "Prediction:\n",
      " [[-0.05624926]\n",
      " [ 0.52908015]\n",
      " [ 0.5675632 ]\n",
      " [ 0.6228093 ]\n",
      " [ 0.522992  ]\n",
      " [ 0.51527834]\n",
      " [ 0.8743818 ]\n",
      " [ 1.0079451 ]]\n",
      "86 Cost:  0.3441677 \n",
      "Prediction:\n",
      " [[-0.05624235]\n",
      " [ 0.5290836 ]\n",
      " [ 0.5675658 ]\n",
      " [ 0.62281084]\n",
      " [ 0.5229946 ]\n",
      " [ 0.51528054]\n",
      " [ 0.87438107]\n",
      " [ 1.0079434 ]]\n",
      "87 Cost:  0.34416518 \n",
      "Prediction:\n",
      " [[-0.05623555]\n",
      " [ 0.5290872 ]\n",
      " [ 0.5675684 ]\n",
      " [ 0.6228124 ]\n",
      " [ 0.522997  ]\n",
      " [ 0.51528263]\n",
      " [ 0.8743804 ]\n",
      " [ 1.0079417 ]]\n",
      "88 Cost:  0.34416267 \n",
      "Prediction:\n",
      " [[-0.05622876]\n",
      " [ 0.52909064]\n",
      " [ 0.56757104]\n",
      " [ 0.62281394]\n",
      " [ 0.5229995 ]\n",
      " [ 0.51528484]\n",
      " [ 0.87437975]\n",
      " [ 1.00794   ]]\n",
      "89 Cost:  0.3441602 \n",
      "Prediction:\n",
      " [[-0.05622196]\n",
      " [ 0.5290942 ]\n",
      " [ 0.5675736 ]\n",
      " [ 0.6228155 ]\n",
      " [ 0.523002  ]\n",
      " [ 0.515287  ]\n",
      " [ 0.87437904]\n",
      " [ 1.0079385 ]]\n",
      "90 Cost:  0.34415764 \n",
      "Prediction:\n",
      " [[-0.05621505]\n",
      " [ 0.5290976 ]\n",
      " [ 0.5675762 ]\n",
      " [ 0.62281704]\n",
      " [ 0.52300453]\n",
      " [ 0.5152892 ]\n",
      " [ 0.8743784 ]\n",
      " [ 1.0079368 ]]\n",
      "91 Cost:  0.34415507 \n",
      "Prediction:\n",
      " [[-0.05620825]\n",
      " [ 0.52910125]\n",
      " [ 0.5675788 ]\n",
      " [ 0.6228186 ]\n",
      " [ 0.523007  ]\n",
      " [ 0.51529133]\n",
      " [ 0.87437767]\n",
      " [ 1.0079352 ]]\n",
      "92 Cost:  0.34415257 \n",
      "Prediction:\n",
      " [[-0.05620146]\n",
      " [ 0.52910477]\n",
      " [ 0.5675814 ]\n",
      " [ 0.62282014]\n",
      " [ 0.5230095 ]\n",
      " [ 0.5152935 ]\n",
      " [ 0.874377  ]\n",
      " [ 1.0079335 ]]\n",
      "93 Cost:  0.3441501 \n",
      "Prediction:\n",
      " [[-0.05619466]\n",
      " [ 0.5291082 ]\n",
      " [ 0.567584  ]\n",
      " [ 0.6228217 ]\n",
      " [ 0.52301204]\n",
      " [ 0.5152957 ]\n",
      " [ 0.87437636]\n",
      " [ 1.007932  ]]\n",
      "94 Cost:  0.34414753 \n",
      "Prediction:\n",
      " [[-0.05618775]\n",
      " [ 0.5291118 ]\n",
      " [ 0.5675866 ]\n",
      " [ 0.62282324]\n",
      " [ 0.5230145 ]\n",
      " [ 0.51529783]\n",
      " [ 0.87437564]\n",
      " [ 1.0079303 ]]\n",
      "95 Cost:  0.344145 \n",
      "Prediction:\n",
      " [[-0.05618095]\n",
      " [ 0.52911526]\n",
      " [ 0.56758916]\n",
      " [ 0.6228248 ]\n",
      " [ 0.5230169 ]\n",
      " [ 0.51530004]\n",
      " [ 0.874375  ]\n",
      " [ 1.0079286 ]]\n",
      "96 Cost:  0.34414253 \n",
      "Prediction:\n",
      " [[-0.05617416]\n",
      " [ 0.52911884]\n",
      " [ 0.5675918 ]\n",
      " [ 0.62282634]\n",
      " [ 0.52301943]\n",
      " [ 0.5153022 ]\n",
      " [ 0.87437433]\n",
      " [ 1.007927  ]]\n",
      "97 Cost:  0.34414 \n",
      "Prediction:\n",
      " [[-0.05616736]\n",
      " [ 0.52912235]\n",
      " [ 0.5675944 ]\n",
      " [ 0.6228279 ]\n",
      " [ 0.52302194]\n",
      " [ 0.5153043 ]\n",
      " [ 0.8743736 ]\n",
      " [ 1.0079253 ]]\n",
      "98 Cost:  0.3441375 \n",
      "Prediction:\n",
      " [[-0.05616045]\n",
      " [ 0.52912587]\n",
      " [ 0.567597  ]\n",
      " [ 0.62282944]\n",
      " [ 0.5230245 ]\n",
      " [ 0.51530653]\n",
      " [ 0.87437296]\n",
      " [ 1.0079237 ]]\n",
      "99 Cost:  0.34413493 \n",
      "Prediction:\n",
      " [[-0.05615366]\n",
      " [ 0.5291294 ]\n",
      " [ 0.5675996 ]\n",
      " [ 0.6228309 ]\n",
      " [ 0.52302694]\n",
      " [ 0.51530874]\n",
      " [ 0.8743723 ]\n",
      " [ 1.007922  ]]\n",
      "100 Cost:  0.34413242 \n",
      "Prediction:\n",
      " [[-0.05614686]\n",
      " [ 0.52913284]\n",
      " [ 0.56760216]\n",
      " [ 0.6228325 ]\n",
      " [ 0.52302945]\n",
      " [ 0.5153108 ]\n",
      " [ 0.8743716 ]\n",
      " [ 1.0079204 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "\n",
    "def min_max_scaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "xy = np.array(\n",
    "    [\n",
    "        [828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "        [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "        [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "        [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "        [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "        [819, 823, 1198100, 816, 820.450012],\n",
    "        [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "        [809.51001, 816.659973, 1398100, 804.539978, 809.559998],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# very important. It does not work without it.\n",
    "xy = min_max_scaler(xy)\n",
    "print(xy)\n",
    "\n",
    "'''\n",
    "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
    " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
    " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
    " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
    " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
    " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
    " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
    " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
    "'''\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        _, cost_val, hy_val = sess.run(\n",
    "            [train, cost, hypothesis], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e41e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
